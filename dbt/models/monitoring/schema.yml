version: 2

models:
  - name: mon_data_quality
    description: "Data quality monitoring tracking metrics across simulation runs"
    columns:
      - name: check_type
        description: "Type of data quality check performed"
        data_tests:
          - not_null
          - accepted_values:
              values: ['RECORD_COUNT', 'COMPLETENESS', 'BUSINESS_RULE', 'OUTLIER_DETECTION', 'YOY_CONSISTENCY', 'FRESHNESS']
      - name: table_name
        description: "Name of the table being monitored"
        data_tests:
          - not_null
      - name: check_dimension
        description: "Dimension or context for the check (e.g., simulation year)"
        data_tests:
          - not_null
      - name: check_subcategory
        description: "Subcategory or specific field being checked"
        data_tests:
          - not_null
      - name: metric_value
        description: "Numeric result of the quality check"
        data_tests:
          - not_null
      - name: quality_status
        description: "Overall status of the quality check"
        data_tests:
          - not_null
          - accepted_values:
              values: ['PASS', 'WARN', 'FAIL']
      - name: check_description
        description: "Human-readable description of what was checked"
        data_tests:
          - not_null
      - name: check_timestamp
        description: "When the quality check was performed"
        data_tests:
          - not_null


  - name: mon_pipeline_performance
    description: "Pipeline performance monitoring tracking execution metrics"
    columns:
      - name: simulation_year
        description: "Simulation year being monitored"
        data_tests:
          - not_null
      - name: total_employees_processed
        description: "Number of employees processed in this simulation"
        data_tests:
          - not_null
          # Temporarily disabled to isolate DuckDBRelation serialization issue
          # - dbt_utils.accepted_range:
          #     min_value: 0
      - name: run_duration_minutes
        description: "Total runtime in minutes"
        data_tests:
          - not_null
          # Temporarily disabled to isolate DuckDBRelation serialization issue
          # - dbt_utils.accepted_range:
          #     min_value: 0
          #     max_value: 1440  # Max 24 hours
      - name: employees_per_minute
        description: "Processing throughput in employees per minute"
        # tests temporarily disabled to isolate DuckDBRelation serialization issue
      - name: overall_performance_category
        description: "Performance categorization"
        data_tests:
          - not_null
          - accepted_values:
              values: ['FAST', 'MEDIUM', 'SLOW']
      - name: overall_quality_score
        description: "Overall data quality score percentage"
        data_tests:
          - not_null
          # Temporarily disabled to isolate DuckDBRelation serialization issue
          # - dbt_utils.accepted_range:
          #     min_value: 0
          #     max_value: 100
          #     inclusive: true
      - name: sla_status
        description: "SLA compliance status"
        data_tests:
          - not_null
          - accepted_values:
              values: ['WITHIN_SLA', 'SLA_WARNING', 'SLA_BREACH']
      - name: performance_alert
        description: "Performance alert category"
        data_tests:
          - not_null
          - accepted_values:
              values: ['OK', 'SLOW_EXECUTION', 'LOW_THROUGHPUT', 'HIGH_CPU_USAGE', 'HIGH_MEMORY_USAGE', 'QUALITY_ISSUE', 'PERFORMANCE_DEGRADATION']
      - name: performance_check_timestamp
        description: "When the performance check was performed"
        data_tests:
          - not_null

  - name: data_quality_promotion_compensation
    description: |
      Comprehensive data quality validation for promotion compensation integrity.
      
      Validates that promotion events use correct previous compensation from the 
      end-of-year workforce snapshot instead of stale baseline data. Identifies
      violations where merit increases have not properly propagated to subsequent
      promotion calculations.
      
      **Critical Business Rules Validated:**
      - Promotion previous_salary must equal workforce_snapshot.current_compensation
      - Merit events must propagate to next year's promotion baseline
      - Compensation gaps indicate data lineage breaks requiring remediation
      
      **Data Quality Statuses:**
      - CRITICAL_VIOLATION: >$5,000 compensation gap (systematic stale data)
      - MAJOR_VIOLATION: >$1,000 compensation gap (significant data issues)  
      - MINOR_VIOLATION: >$100 compensation gap (minor discrepancies)
      - WARNING: >$10 compensation gap (investigate)
      - PASS: Acceptable compensation continuity (<$10 gap)
    columns:
      - name: employee_id
        description: Employee identifier
        data_tests:
          - not_null
      - name: simulation_year
        description: Simulation year for the promotion event
        data_tests:
          - not_null
      - name: data_quality_status
        description: Data quality assessment (CRITICAL_VIOLATION, MAJOR_VIOLATION, MINOR_VIOLATION, WARNING, PASS)
        data_tests:
          - not_null
          - accepted_values:
              values: ['CRITICAL_VIOLATION', 'MAJOR_VIOLATION', 'MINOR_VIOLATION', 'WARNING', 'PASS']
      - name: compensation_gap
        description: Difference between promotion previous salary and actual previous year compensation
      - name: gap_percentage
        description: Compensation gap as percentage of actual compensation
      - name: merit_propagation_status
        description: Whether merit events properly propagated to promotion calculations
        data_tests:
          - accepted_values:
              values: ['MERIT_NOT_PROPAGATED', 'MERIT_PROPERLY_PROPAGATED', 'NO_MERIT_EVENT']
      - name: estimated_underpayment_amount
        description: Estimated financial impact of compensation gap (if employee was underpaid)
        data_tests:
          - not_null
      - name: requires_immediate_attention
        description: Flag for violations requiring immediate remediation
        data_tests:
          - not_null

  - name: data_quality_summary  
    description: |
      Executive dashboard summary of promotion compensation data quality metrics.
      
      Provides aggregated KPIs and trend analysis for data quality monitoring.
      Used by Dagster asset checks and Streamlit dashboards for real-time
      data quality monitoring and compliance reporting.
      
      **Key Metrics:**
      - Data Quality Score: 0-100 score based on violation severity
      - Compliance Status: COMPLIANT, MINOR_ISSUES, MODERATE_ISSUES, CRITICAL_ISSUES
      - Financial Risk Level: Based on estimated underpayment amounts
      - Violation Rates: Percentage of promotions with data quality issues
    columns:
      - name: simulation_year
        description: Simulation year or 'ALL_YEARS' for aggregate metrics
        data_tests:
          - not_null
      - name: total_promotions
        description: Total number of promotion events
        data_tests:
          - not_null
      - name: data_quality_score
        description: Overall data quality score (0-100)
      - name: compliance_status
        description: Enterprise compliance assessment
        data_tests:
          - accepted_values:
              values: ['COMPLIANT', 'MINOR_ISSUES', 'MODERATE_ISSUES', 'CRITICAL_ISSUES']
      - name: financial_risk_level
        description: Financial risk assessment based on underpayment amounts
        data_tests:
          - accepted_values:
              values: ['HIGH_RISK', 'MEDIUM_RISK', 'LOW_RISK', 'MINIMAL_RISK']
      - name: total_estimated_underpayment
        description: Total estimated financial impact across all violations
