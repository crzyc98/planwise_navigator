version: 2

models:
  - name: dq_employee_id_validation
    description: >
      Data quality validation model for employee ID integrity.
      Checks for duplicates, format violations, and SSN conflicts across all workforce data.
    columns:
      - name: check_type
        description: Type of data quality check performed
        data_tests:
          - not_null
          - accepted_values:
              values: ['DUPLICATE_IDS', 'INVALID_FORMAT', 'LEGACY_FORMAT', 'SSN_SHARED']
      - name: severity
        description: Severity level of the issue (ERROR, WARNING, INFO)
        data_tests:
          - not_null
          - accepted_values:
              values: ['ERROR', 'WARNING', 'INFO']
      - name: issue_count
        description: Number of records with this specific issue
        data_tests:
          - not_null
      - name: description
        description: Human-readable description of the issue
        data_tests:
          - not_null
      - name: details
        description: JSON array containing detailed information about each issue

    data_tests:
      - dbt_utils.expression_is_true:
          expression: "issue_count = 0"
          where: "severity = 'ERROR'"
          error_if: ">0"
          warn_if: "=0"
          meta:
            description: "Fail if any ERROR-level data quality issues are found"

  - name: dq_employee_contributions_validation
    description: >
      Data quality validation for employee contribution calculations in Epic E034.
      Validates contributions don't exceed compensation, checks rate consistency,
      validates IRS 402(g) limits, and ensures data integrity. Returns only failing
      records for review - empty result indicates all validations passed.
    config:
      tags: ["data_quality", "contributions", "epic_e034", "validation"]
    columns:
      - name: employee_id
        description: "Employee identifier with validation failure"
        data_tests:
          - not_null
      - name: simulation_year
        description: "Simulation year for the validation"
        data_tests:
          - not_null
          - dbt_utils.accepted_range:
              min_value: 2020
              max_value: 2050
      - name: validation_rule
        description: "Type of validation that failed"
        data_tests:
          - not_null
          - accepted_values:
              values:
                - 'contributions_exceed_compensation'
                - 'deferral_rate_inconsistency'
                - 'irs_402g_limit_exceeded'
                - 'contribution_components_mismatch'
                - 'negative_contribution_amount'
                - 'enrolled_without_contributions'
                - 'excessive_contribution_rate'
                - 'irs_limit_flag_inaccurate'
      - name: severity
        description: "Severity level of the validation failure"
        data_tests:
          - not_null
          - accepted_values:
              values: ['ERROR', 'WARNING', 'INFO']
      - name: validation_message
        description: "Human-readable description of the validation failure"
        data_tests:
          - not_null
      - name: actual_value
        description: "Actual value that caused the validation failure"
      - name: expected_max_value
        description: "Expected or maximum allowed value"
      - name: variance
        description: "Difference between actual and expected value"
      - name: severity_rank
        description: "Numeric ranking of severity (1=ERROR, 2=WARNING, 3=INFO)"
        data_tests:
          - not_null
          - accepted_values:
              values: [1, 2, 3]
      - name: validation_category
        description: "Category grouping of validation rules"
        data_tests:
          - not_null
          - accepted_values:
              values:
                - 'CONTRIBUTION_AMOUNTS'
                - 'RATE_VALIDATION'
                - 'IRS_COMPLIANCE'
                - 'DATA_INTEGRITY'
                - 'ENROLLMENT_CONSISTENCY'
                - 'OTHER'
      - name: validation_timestamp
        description: "When the validation was performed"
        data_tests:
          - not_null
      - name: validation_source
        description: "Source identifier for the validation"
        data_tests:
          - not_null

    data_tests:
      # Critical test: No ERROR-level contribution validation failures should exist
      - dbt_utils.expression_is_true:
          expression: "1=1"  # This model returns only failures, so any ERROR record is a problem
          where: "severity = 'ERROR'"
          error_if: ">0"
          warn_if: "=0"
          name: "no_critical_contribution_validation_failures"
          meta:
            description: "Fail if any ERROR-level contribution validation issues are found"
      # Warning test: Limit WARNING-level failures to reasonable threshold
      - dbt_utils.expression_is_true:
          expression: "1=1"
          where: "severity = 'WARNING'"
          warn_if: ">10"  # Warn if more than 10 WARNING-level issues
          name: "warning_contribution_validation_failures_threshold"
          meta:
            description: "Warn if excessive WARNING-level contribution validation issues are found"

  - name: dq_deferral_escalation_validation
    description: >
      Comprehensive data quality validation for Epic E035 automatic deferral rate escalation system.
      Provides real-time monitoring with automated health scoring (0-100 scale) covering business
      rule enforcement, integration consistency, event sequencing, parameter validation, and
      multi-year progression tracking. Critical for ensuring production readiness and compliance.
    config:
      tags: ["data_quality", "escalation", "monitoring", "epic_e035"]
    columns:
      - name: simulation_year
        description: Simulation year being validated
        data_tests:
          - not_null
          - dbt_utils.accepted_range:
              min_value: 2025
              max_value: 2050
      - name: health_score
        description: Overall system health score (0-100, where 100 = perfect)
        data_tests:
          - not_null
          - dbt_utils.accepted_range:
              min_value: 0
              max_value: 100
      - name: health_status
        description: Categorical health status based on health score
        data_tests:
          - not_null
          - accepted_values:
              values: ['PERFECT', 'EXCELLENT', 'GOOD', 'FAIR', 'POOR', 'CRITICAL']
      - name: total_violations
        description: Total number of data quality violations detected
        data_tests:
          - not_null
          - dbt_utils.accepted_range:
              min_value: 0
              max_value: 100000
      - name: total_records
        description: Total number of records examined across all validations
        data_tests:
          - not_null
          - dbt_utils.accepted_range:
              min_value: 0
              max_value: 1000000
      - name: violation_rate_pct
        description: Percentage of records with violations
        data_tests:
          - not_null
          - dbt_utils.accepted_range:
              min_value: 0
              max_value: 100
      - name: invalid_deferral_rates
        description: Count of invalid deferral rates (outside 0-100% range)
        data_tests:
          - not_null
      - name: duplicate_escalations
        description: Count of duplicate escalation events (should be 0)
        data_tests:
          - not_null
      - name: incorrect_effective_dates
        description: Count of escalations not on January 1st (violates user requirement)
        data_tests:
          - not_null
      - name: deferral_rate_mismatches
        description: Count of deferral rate inconsistencies between models
        data_tests:
          - not_null
      - name: escalation_count_decreases
        description: Count of employees with decreasing escalation counts year-over-year
        data_tests:
          - not_null
      - name: recommendations
        description: System-generated recommendations based on health score
        data_tests:
          - not_null
      - name: validation_timestamp
        description: Timestamp when validation was performed
        data_tests:
          - not_null

    data_tests:
      # Critical test: Health score must be above minimum threshold for production
      - dbt_utils.expression_is_true:
          expression: "health_score >= 85"
          error_if: "<85"
          warn_if: "<95"
          name: "escalation_system_health_score_threshold"
          meta:
            description: "Ensure escalation system health score meets production standards"

      # Critical test: No duplicate escalation events allowed
      - dbt_utils.expression_is_true:
          expression: "duplicate_escalations = 0"
          error_if: ">0"
          name: "no_duplicate_escalation_events"
          meta:
            description: "Fail if duplicate escalation events are detected"

      # Critical test: All escalations must be on January 1st per user requirement
      - dbt_utils.expression_is_true:
          expression: "incorrect_effective_dates = 0"
          error_if: ">0"
          name: "escalations_on_january_first"
          meta:
            description: "Fail if escalation events are not on January 1st as required"

      # Critical test: No deferral rate mismatches between models
      - dbt_utils.expression_is_true:
          expression: "deferral_rate_mismatches = 0"
          error_if: ">0"
          warn_if: ">5"
          name: "no_deferral_rate_mismatches"
          meta:
            description: "Ensure deferral rates are consistent across all models"

      # Critical test: Escalation counts should never decrease year-over-year
      - dbt_utils.expression_is_true:
          expression: "escalation_count_decreases = 0"
          error_if: ">0"
          name: "no_escalation_count_decreases"
          meta:
            description: "Fail if escalation counts decrease between simulation years"
