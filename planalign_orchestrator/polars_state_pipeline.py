#!/usr/bin/env python3
"""
E076 Polars State Accumulation Pipeline - Maximum Performance State Processing

High-performance alternative to dbt-based state accumulation using Polars for
vectorized operations. Achieves 60-75% overall runtime improvement by replacing
the 70% bottleneck (state accumulation) with in-memory columnar processing.

This module provides:
- StateAccumulatorEngine: Core engine for state transformations
- EnrollmentStateBuilder: Temporal enrollment state tracking
- DeferralRateBuilder: Temporal deferral rate state tracking
- ContributionsCalculator: Vectorized contribution calculations
- SnapshotBuilder: Final workforce snapshot generation

Architecture:
    Events (Parquet) → State Accumulators → Contributions → Snapshot → DuckDB

Performance Target:
    State Accumulation: 20-25s → 2-5s (80-90% reduction)
    Total Runtime: 236s → 60-90s (60-75% improvement)
"""

import os
import sys
import json
import time
import logging
import polars as pl
import duckdb
from pathlib import Path
from datetime import date, datetime, timedelta
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Union

# Set Polars to use maximum threads for performance
os.environ.setdefault('POLARS_MAX_THREADS', '16')

# Import project modules
sys.path.insert(0, str(Path(__file__).parent.parent))
from planalign_orchestrator.config import load_simulation_config, get_database_path

# Module-level logger
logger = logging.getLogger(__name__)


@dataclass
class StateAccumulatorConfig:
    """Configuration for Polars state accumulation."""
    simulation_year: int
    scenario_id: str = "default"
    plan_design_id: str = "default"
    database_path: Optional[Path] = None  # If None, uses get_database_path()
    events_path: Optional[Path] = None  # Path to Parquet events directory
    output_path: Optional[Path] = None  # Output path for state data

    # Performance optimization settings
    enable_validation: bool = True  # Validate against dbt output
    enable_profiling: bool = False
    lazy_evaluation: bool = True
    streaming: bool = True

    def __post_init__(self):
        """Validate configuration after initialization."""
        if self.simulation_year < 2025:
            raise ValueError(f"simulation_year must be >= 2025, got {self.simulation_year}")


class StateAccumulatorEngine:
    """
    Core engine for Polars-based state accumulation.

    Replaces dbt state accumulation with vectorized in-memory operations
    for 80-90% performance improvement on the bottleneck stage.

    Architecture:
        1. Load events from Parquet (already generated by Polars or SQL)
        2. Build temporal state accumulators (enrollment, deferral rate)
        3. Calculate contributions using vectorized operations
        4. Generate final workforce snapshot
        5. Write results to DuckDB for dbt consumption
    """

    def __init__(self, config: StateAccumulatorConfig):
        """Initialize state accumulator engine with configuration."""
        self.config = config
        self.logger = self._setup_logging()

        # Performance monitoring
        self.start_time = time.time()
        self.stats = {
            'total_processing_time': 0.0,
            'enrollment_state_time': 0.0,
            'deferral_state_time': 0.0,
            'contributions_time': 0.0,
            'snapshot_time': 0.0,
            'employees_processed': 0,
            'events_processed': 0
        }

        # Resolve database path
        self.db_path = self.config.database_path if self.config.database_path else get_database_path()

        self.logger.info(f"Initialized StateAccumulatorEngine for year {config.simulation_year}")
        self.logger.info(f"Database path: {self.db_path}")

    def _setup_logging(self) -> logging.Logger:
        """Setup logging with appropriate level and formatting."""
        logger = logging.getLogger(__name__)

        # Don't add handlers if already configured
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)

        return logger

    def _load_events(self) -> pl.DataFrame:
        """
        Load events for current simulation year from Parquet or DuckDB.

        Tries multiple sources in order of preference:
        1. Parquet files (fastest - from Polars event generation)
        2. DuckDB fct_yearly_events table (fallback for SQL-generated events)

        Returns:
            Polars DataFrame with events for the current simulation year
        """
        # Try Parquet first (fastest path for Polars-generated events)
        if self.config.events_path:
            parquet_path = self.config.events_path / f"simulation_year={self.config.simulation_year}"
            if parquet_path.exists():
                self.logger.info(f"Loading events from Parquet: {parquet_path}")
                events_df = pl.read_parquet(parquet_path / f"events_{self.config.simulation_year}.parquet")
                self.stats['events_processed'] = events_df.height
                return events_df

        # Fallback to DuckDB
        self.logger.info(f"Loading events from DuckDB: {self.db_path}")
        if not self.db_path.exists():
            raise FileNotFoundError(f"Database not found: {self.db_path}")

        conn = duckdb.connect(str(self.db_path), read_only=True)
        query = f"""
            SELECT *
            FROM fct_yearly_events
            WHERE simulation_year = {self.config.simulation_year}
              AND scenario_id = '{self.config.scenario_id}'
        """
        events_df = conn.execute(query).pl()
        conn.close()

        self.stats['events_processed'] = events_df.height
        self.logger.info(f"Loaded {events_df.height} events from DuckDB")
        return events_df

    def _load_baseline_workforce(self) -> pl.DataFrame:
        """
        Load baseline workforce for Year 1 or previous year snapshot for Year 2+.

        Returns:
            Polars DataFrame with baseline workforce data
        """
        conn = duckdb.connect(str(self.db_path), read_only=True)

        # Year 1: Load from staging
        if self.config.simulation_year == 2025:  # TODO: Make this configurable
            self.logger.info("Year 1: Loading baseline workforce from stg_census_data")
            query = """
                SELECT
                    employee_id,
                    employee_ssn,
                    employee_birth_date,
                    employee_hire_date,
                    employee_gross_compensation,
                    employee_deferral_rate,
                    current_eligibility_status,
                    employee_enrollment_date,
                    active
                FROM stg_census_data
                WHERE active = true
            """
        else:
            # Year 2+: Load from previous year's snapshot
            # Note: fct_workforce_snapshot does not have scenario_id column
            prev_year = self.config.simulation_year - 1
            self.logger.info(f"Year {self.config.simulation_year}: Loading from Year {prev_year} snapshot")
            query = f"""
                SELECT
                    employee_id,
                    employee_ssn,
                    employee_birth_date,
                    employee_hire_date,
                    current_compensation as employee_gross_compensation,
                    effective_annual_deferral_rate as employee_deferral_rate,
                    current_eligibility_status,
                    employee_enrollment_date,
                    CASE WHEN employment_status = 'active' THEN true ELSE false END as active
                FROM fct_workforce_snapshot
                WHERE simulation_year = {prev_year}
                  AND employment_status = 'active'
            """

        baseline_df = conn.execute(query).pl()
        conn.close()

        self.stats['employees_processed'] = baseline_df.height
        self.logger.info(f"Loaded {baseline_df.height} baseline employees")
        return baseline_df

    def build_state(self, previous_enrollment_state: Optional[pl.DataFrame] = None,
                     previous_deferral_state: Optional[pl.DataFrame] = None) -> Dict[str, pl.DataFrame]:
        """
        Build all state accumulators for the current simulation year.

        Orchestrates the entire state accumulation pipeline:
        1. Load events and baseline workforce
        2. Build enrollment state
        3. Build deferral rate state
        4. Calculate contributions
        5. Generate workforce snapshot

        Args:
            previous_enrollment_state: Previous year's enrollment state (Year 2+)
            previous_deferral_state: Previous year's deferral state (Year 2+)

        Returns:
            Dictionary containing all state DataFrames
        """
        total_start = time.time()
        self.logger.info(f"Building state for year {self.config.simulation_year}...")

        # Load input data
        events_df = self._load_events()
        baseline_df = self._load_baseline_workforce()

        # Initialize builders
        enrollment_builder = EnrollmentStateBuilder(self.logger)
        deferral_builder = DeferralRateBuilder(self.logger)
        contributions_calculator = ContributionsCalculator(self.logger)
        snapshot_builder = SnapshotBuilder(self.logger)

        # Build enrollment state (S076-02)
        enrollment_start = time.time()
        enrollment_state = enrollment_builder.build(
            simulation_year=self.config.simulation_year,
            events_df=events_df,
            baseline_df=baseline_df,
            previous_state_df=previous_enrollment_state
        )
        self.stats['enrollment_state_time'] = time.time() - enrollment_start
        self.logger.info(f"Enrollment state built in {self.stats['enrollment_state_time']:.3f}s")

        # Build deferral rate state (S076-02)
        deferral_start = time.time()
        deferral_state = deferral_builder.build(
            simulation_year=self.config.simulation_year,
            events_df=events_df,
            enrollment_state_df=enrollment_state,
            baseline_df=baseline_df,
            previous_state_df=previous_deferral_state
        )
        self.stats['deferral_state_time'] = time.time() - deferral_start
        self.logger.info(f"Deferral state built in {self.stats['deferral_state_time']:.3f}s")

        # Calculate contributions (S076-03)
        contributions_start = time.time()
        contributions = contributions_calculator.calculate(
            simulation_year=self.config.simulation_year,
            enrollment_state_df=enrollment_state,
            deferral_state_df=deferral_state,
            baseline_df=baseline_df,
            events_df=events_df
        )
        self.stats['contributions_time'] = time.time() - contributions_start
        self.logger.info(f"Contributions calculated in {self.stats['contributions_time']:.3f}s")

        # Generate snapshot (S076-04)
        snapshot_start = time.time()
        snapshot = snapshot_builder.build(
            simulation_year=self.config.simulation_year,
            baseline_df=baseline_df,
            events_df=events_df,
            enrollment_state_df=enrollment_state,
            deferral_state_df=deferral_state,
            contributions_df=contributions
        )
        self.stats['snapshot_time'] = time.time() - snapshot_start
        self.logger.info(f"Snapshot built in {self.stats['snapshot_time']:.3f}s")

        total_time = time.time() - total_start
        self.stats['total_processing_time'] = total_time

        self.logger.info(f"State accumulation complete in {total_time:.2f}s")
        self.logger.info(f"Processed {self.stats['employees_processed']} employees, {self.stats['events_processed']} events")

        return {
            'events': events_df,
            'baseline': baseline_df,
            'enrollment_state': enrollment_state,
            'deferral_state': deferral_state,
            'contributions': contributions,
            'snapshot': snapshot
        }

    def write_to_database(self, state_data: Dict[str, pl.DataFrame]) -> None:
        """
        Write state data to DuckDB for dbt consumption.

        E096 FIX: Write to the same table names that dbt models use, so downstream
        models (like fct_workforce_snapshot) can seamlessly consume Polars output.

        Table mapping:
        - enrollment_state → int_enrollment_state_accumulator
        - deferral_state → int_deferral_rate_state_accumulator_v2
        - contributions → int_employee_contributions
        - snapshot → (written by dbt post-processing)

        Args:
            state_data: Dictionary of state DataFrames to write
        """
        conn = duckdb.connect(str(self.db_path))

        try:
            # Write enrollment state to int_enrollment_state_accumulator
            if 'enrollment_state' in state_data and state_data['enrollment_state'].height > 0:
                enrollment_df = state_data['enrollment_state']
                # Delete existing data for this year and write new
                conn.execute(f"""
                    CREATE TABLE IF NOT EXISTS int_enrollment_state_accumulator AS
                    SELECT * FROM enrollment_df WHERE 1=0
                """)
                conn.execute(f"""
                    DELETE FROM int_enrollment_state_accumulator
                    WHERE simulation_year = {self.config.simulation_year}
                """)
                conn.execute(f"""
                    INSERT INTO int_enrollment_state_accumulator
                    SELECT * FROM enrollment_df
                """)
                self.logger.info(f"Wrote {enrollment_df.height} enrollment state records to int_enrollment_state_accumulator")

            # Write deferral state to int_deferral_rate_state_accumulator_v2
            if 'deferral_state' in state_data and state_data['deferral_state'].height > 0:
                deferral_df = state_data['deferral_state']
                conn.execute(f"""
                    CREATE TABLE IF NOT EXISTS int_deferral_rate_state_accumulator_v2 AS
                    SELECT * FROM deferral_df WHERE 1=0
                """)
                conn.execute(f"""
                    DELETE FROM int_deferral_rate_state_accumulator_v2
                    WHERE simulation_year = {self.config.simulation_year}
                """)
                conn.execute(f"""
                    INSERT INTO int_deferral_rate_state_accumulator_v2
                    SELECT * FROM deferral_df
                """)
                self.logger.info(f"Wrote {deferral_df.height} deferral state records to int_deferral_rate_state_accumulator_v2")

            # Write contributions to int_employee_contributions
            if 'contributions' in state_data and state_data['contributions'].height > 0:
                contributions_df = state_data['contributions']
                conn.execute(f"""
                    CREATE TABLE IF NOT EXISTS int_employee_contributions AS
                    SELECT * FROM contributions_df WHERE 1=0
                """)
                conn.execute(f"""
                    DELETE FROM int_employee_contributions
                    WHERE simulation_year = {self.config.simulation_year}
                """)
                conn.execute(f"""
                    INSERT INTO int_employee_contributions
                    SELECT * FROM contributions_df
                """)
                self.logger.info(f"Wrote {contributions_df.height} contribution records to int_employee_contributions")

            # Note: snapshot is written by fct_workforce_snapshot dbt model in post-processing
            # We only log if snapshot data was generated but will be used by dbt
            if 'snapshot' in state_data and state_data['snapshot'].height > 0:
                self.logger.info(f"Generated {state_data['snapshot'].height} snapshot records (will be written by dbt post-processing)")

            conn.commit()
            self.logger.info("State data written to database successfully")

        except Exception as e:
            self.logger.error(f"Error writing state data to database: {e}")
            conn.rollback()
            raise
        finally:
            conn.close()

    def validate_against_dbt(self, state_data: Dict[str, pl.DataFrame]) -> Dict[str, Any]:
        """
        Validate Polars state data against dbt output for parity testing.

        Args:
            state_data: Dictionary of state DataFrames from Polars

        Returns:
            Dictionary with validation results
        """
        results = {
            'passed': True,
            'validations': [],
            'errors': []
        }

        if not self.config.enable_validation:
            self.logger.info("Validation disabled, skipping parity check")
            return results

        conn = duckdb.connect(str(self.db_path), read_only=True)

        try:
            # Validate snapshot row counts
            if 'snapshot' in state_data:
                polars_count = state_data['snapshot'].height
                try:
                    dbt_count = conn.execute(f"""
                        SELECT COUNT(*) FROM fct_workforce_snapshot
                        WHERE simulation_year = {self.config.simulation_year}
                    """).fetchone()[0]

                    if polars_count == dbt_count:
                        results['validations'].append({
                            'test': 'snapshot_row_count',
                            'passed': True,
                            'polars': polars_count,
                            'dbt': dbt_count
                        })
                    else:
                        results['validations'].append({
                            'test': 'snapshot_row_count',
                            'passed': False,
                            'polars': polars_count,
                            'dbt': dbt_count,
                            'diff': abs(polars_count - dbt_count)
                        })
                        results['passed'] = False
                except Exception as e:
                    results['errors'].append(f"Could not validate snapshot: {e}")

            # Validate contributions
            if 'contributions' in state_data:
                polars_contrib_count = state_data['contributions'].height
                try:
                    dbt_contrib_count = conn.execute(f"""
                        SELECT COUNT(*) FROM int_employee_contributions
                        WHERE simulation_year = {self.config.simulation_year}
                    """).fetchone()[0]

                    results['validations'].append({
                        'test': 'contributions_row_count',
                        'passed': polars_contrib_count == dbt_contrib_count,
                        'polars': polars_contrib_count,
                        'dbt': dbt_contrib_count
                    })
                except Exception as e:
                    results['errors'].append(f"Could not validate contributions: {e}")

        except Exception as e:
            results['errors'].append(f"Validation error: {e}")
            results['passed'] = False
        finally:
            conn.close()

        return results


class EnrollmentStateBuilder:
    """
    Builds temporal enrollment state across simulation years.

    Replaces: int_enrollment_state_accumulator.sql
    Performance Target: <500ms per builder

    Architecture:
        - Year 1: Baseline workforce + current year events
        - Year 2+: Previous year state + current year events
        - Tracks: enrollment_date, enrollment_status, enrollment_method
        - Handles: enrollments, opt-outs, re-enrollments
    """

    def __init__(self, logger: logging.Logger):
        """Initialize enrollment state builder."""
        self.logger = logger

    def build(
        self,
        simulation_year: int,
        events_df: pl.DataFrame,
        baseline_df: pl.DataFrame,
        previous_state_df: Optional[pl.DataFrame] = None
    ) -> pl.DataFrame:
        """
        Build enrollment state for the simulation year.

        Args:
            simulation_year: Current simulation year
            events_df: All events for current year
            baseline_df: Baseline workforce (Year 1 only)
            previous_state_df: Previous year's enrollment state (Year 2+)

        Returns:
            DataFrame with enrollment state for current year
        """
        start_time = time.time()

        # Extract enrollment events for current year
        enrollment_events = events_df.filter(
            pl.col('event_type').is_in(['enrollment', 'enrollment_change'])
        )

        if enrollment_events.height == 0:
            self.logger.debug(f"No enrollment events for year {simulation_year}")

        # Consolidate current year enrollment events
        current_year_summary = self._consolidate_current_year_events(
            enrollment_events, simulation_year
        )

        # Year 1: Use baseline + events
        if previous_state_df is None:
            enrollment_state = self._build_year1_state(
                baseline_df, current_year_summary, simulation_year
            )
        else:
            # Year 2+: Use previous state + events
            enrollment_state = self._build_subsequent_year_state(
                previous_state_df, current_year_summary, simulation_year
            )

        elapsed = time.time() - start_time
        self.logger.info(
            f"Built enrollment state for {enrollment_state.height} employees "
            f"in {elapsed:.3f}s"
        )

        return enrollment_state

    def _consolidate_current_year_events(
        self, enrollment_events: pl.DataFrame, simulation_year: int
    ) -> pl.DataFrame:
        """
        Consolidate enrollment events for current year.

        Handles multiple events per employee, prioritizing latest event.
        """
        if enrollment_events.height == 0:
            return pl.DataFrame()

        # Add event priority (latest event = priority 1)
        events_with_priority = enrollment_events.with_columns([
            pl.col('effective_date').rank('ordinal', descending=True)
            .over(['employee_id', 'event_type'])
            .alias('event_priority')
        ])

        # Parse enrollment information
        parsed_events = events_with_priority.with_columns([
            # Enrollment date
            pl.when(pl.col('event_type') == 'enrollment')
            .then(pl.col('effective_date'))
            .otherwise(None)
            .alias('new_enrollment_date'),

            # Enrollment status change
            pl.when(pl.col('event_type') == 'enrollment')
            .then(pl.lit(True))
            .when(
                (pl.col('event_type') == 'enrollment_change') &
                (pl.col('event_details').str.to_lowercase().str.contains('opt-out') |
                 pl.col('event_details').str.to_lowercase().str.contains('opted out'))
            )
            .then(pl.lit(False))
            .otherwise(None)
            .alias('enrollment_status_change'),

            # Enrollment method
            pl.when(
                (pl.col('event_type') == 'enrollment') &
                (pl.col('event_category') == 'auto_enrollment')
            )
            .then(pl.lit('auto'))
            .when(
                (pl.col('event_type') == 'enrollment') &
                pl.col('event_category').is_in([
                    'voluntary_enrollment',
                    'proactive_enrollment',
                    'executive_enrollment'
                ])
            )
            .then(pl.lit('voluntary'))
            .otherwise(None)
            .alias('enrollment_method'),

            # Opt-out flag
            pl.when(
                (pl.col('event_type') == 'enrollment_change') &
                (pl.col('event_details').str.to_lowercase().str.contains('opt-out') |
                 pl.col('event_details').str.to_lowercase().str.contains('opted out'))
            )
            .then(pl.lit(True))
            .otherwise(pl.lit(False))
            .alias('is_opt_out_event')
        ])

        # Aggregate by employee (use all events for counting, not just priority 1)
        summary = parsed_events.group_by('employee_id').agg([
            pl.lit(simulation_year).alias('simulation_year'),

            # Latest enrollment event date (from priority 1 events only)
            pl.col('new_enrollment_date')
            .filter((pl.col('event_type') == 'enrollment') & (pl.col('event_priority') == 1))
            .max()
            .alias('enrollment_event_date'),

            # Enrollment method (from priority 1 enrollment event)
            pl.col('enrollment_method')
            .filter((pl.col('event_type') == 'enrollment') & (pl.col('event_priority') == 1))
            .max()
            .alias('enrollment_method_this_year'),

            # Final enrollment status (opt-out overrides enrollment)
            # Check if ANY opt-out event exists (not just priority 1)
            pl.when(
                pl.col('is_opt_out_event').sum() > 0
            )
            .then(pl.lit(False))
            .when(
                pl.col('event_type')
                .filter(pl.col('event_type') == 'enrollment')
                .count() > 0
            )
            .then(pl.lit(True))
            .otherwise(None)
            .alias('has_enrollment_event_this_year'),

            # Opt-out tracking (any opt-out event)
            (pl.col('is_opt_out_event').sum() > 0).alias('had_opt_out_this_year'),

            # Event counts (count ALL events, not just priority 1)
            pl.col('event_type')
            .filter(pl.col('event_type') == 'enrollment')
            .count()
            .alias('enrollment_events_count'),

            pl.col('event_type')
            .filter(pl.col('event_type') == 'enrollment_change')
            .count()
            .alias('enrollment_change_events_count')
        ])

        return summary

    def _build_year1_state(
        self,
        baseline_df: pl.DataFrame,
        current_year_summary: pl.DataFrame,
        simulation_year: int
    ) -> pl.DataFrame:
        """Build enrollment state for Year 1 from baseline + events."""
        # Baseline enrollment state
        baseline_state = baseline_df.select([
            pl.col('employee_id'),
            pl.lit(simulation_year).alias('simulation_year'),
            pl.col('employee_enrollment_date').alias('baseline_enrollment_date'),
            pl.when(pl.col('employee_enrollment_date').is_not_null())
            .then(pl.lit(True))
            .otherwise(pl.lit(False))
            .alias('baseline_enrollment_status'),
            pl.lit(0).alias('years_since_first_enrollment'),
            pl.lit('baseline').alias('enrollment_source')
        ])

        # Combine baseline with current year events
        if current_year_summary.height == 0:
            # No events, just baseline
            return baseline_state.select([
                'employee_id',
                'simulation_year',
                pl.col('baseline_enrollment_date').alias('enrollment_date'),
                pl.col('baseline_enrollment_status').alias('enrollment_status'),
                'years_since_first_enrollment',
                'enrollment_source',
                pl.lit(None, dtype=pl.Utf8).alias('enrollment_method'),
                pl.lit(False).alias('ever_opted_out'),
                pl.lit(False).alias('ever_unenrolled'),
                pl.lit(0).alias('enrollment_events_this_year'),
                pl.lit(0).alias('enrollment_change_events_this_year'),
                # Additional columns to match dbt schema (E097 fix)
                pl.when(pl.col('baseline_enrollment_status') == True)
                .then(pl.col('baseline_enrollment_date'))
                .otherwise(None)
                .alias('effective_enrollment_date'),
                pl.when(pl.col('baseline_enrollment_status') == True)
                .then(pl.lit(True))
                .otherwise(pl.lit(False))
                .alias('is_enrolled'),
                pl.lit(datetime.now()).alias('created_at'),
                pl.lit('default').alias('scenario_id'),
                pl.lit('VALID').alias('data_quality_flag')
            ])

        # Join baseline with events
        combined = baseline_state.join(
            current_year_summary,
            on='employee_id',
            how='left'  # Use left join to keep all baseline employees
        ).with_columns([
            # Effective enrollment date (events override baseline)
            pl.when(pl.col('enrollment_event_date').is_not_null())
            .then(pl.col('enrollment_event_date'))
            .when(pl.col('baseline_enrollment_date').is_not_null())
            .then(pl.col('baseline_enrollment_date'))
            .otherwise(None)
            .alias('enrollment_date'),

            # Effective enrollment status
            pl.when(pl.col('has_enrollment_event_this_year').is_not_null())
            .then(pl.col('has_enrollment_event_this_year'))
            .otherwise(pl.col('baseline_enrollment_status'))
            .alias('enrollment_status'),

            # Years since enrollment (0 for first year)
            pl.when(
                pl.col('enrollment_event_date').is_not_null() |
                pl.col('baseline_enrollment_date').is_not_null()
            )
            .then(pl.lit(0))
            .otherwise(None)
            .alias('years_since_first_enrollment'),

            # Enrollment source
            pl.when(pl.col('enrollment_event_date').is_not_null())
            .then(pl.lit(f'event_{simulation_year}'))
            .otherwise(pl.col('enrollment_source'))  # Preserve baseline source
            .alias('enrollment_source'),

            # Enrollment method
            pl.when(pl.col('enrollment_method_this_year').is_not_null())
            .then(pl.col('enrollment_method_this_year'))
            .otherwise(None)
            .alias('enrollment_method'),

            # Opt-out tracking
            pl.col('had_opt_out_this_year').fill_null(False).alias('ever_opted_out'),
            pl.lit(False).alias('ever_unenrolled'),

            # Event counts
            pl.col('enrollment_events_count').fill_null(0).alias('enrollment_events_this_year'),
            pl.col('enrollment_change_events_count').fill_null(0).alias('enrollment_change_events_this_year')
        ])

        return combined.select([
            'employee_id',
            pl.col('simulation_year').fill_null(simulation_year),
            'enrollment_date',
            'enrollment_status',
            'years_since_first_enrollment',
            'enrollment_source',
            'enrollment_method',
            'ever_opted_out',
            'ever_unenrolled',
            'enrollment_events_this_year',
            'enrollment_change_events_this_year',
            # Additional columns to match dbt schema (E097 fix)
            pl.when(pl.col('enrollment_status') == True)
            .then(pl.col('enrollment_date'))
            .otherwise(None)
            .alias('effective_enrollment_date'),
            pl.when(pl.col('enrollment_status') == True)
            .then(pl.lit(True))
            .otherwise(pl.lit(False))
            .alias('is_enrolled'),
            pl.lit(datetime.now()).alias('created_at'),
            pl.lit('default').alias('scenario_id'),
            pl.when(pl.col('employee_id').is_null())
            .then(pl.lit('INVALID_EMPLOYEE_ID'))
            .when(pl.col('simulation_year').is_null())
            .then(pl.lit('INVALID_SIMULATION_YEAR'))
            .when(pl.col('enrollment_status').is_null())
            .then(pl.lit('INVALID_ENROLLMENT_STATUS'))
            .otherwise(pl.lit('VALID'))
            .alias('data_quality_flag')
        ])

    def _build_subsequent_year_state(
        self,
        previous_state_df: pl.DataFrame,
        current_year_summary: pl.DataFrame,
        simulation_year: int
    ) -> pl.DataFrame:
        """Build enrollment state for Year 2+ from previous state + events."""
        # Prepare previous year state
        previous = previous_state_df.select([
            pl.col('employee_id'),
            pl.col('enrollment_date').alias('previous_enrollment_date'),
            pl.col('enrollment_status').alias('previous_enrollment_status'),
            pl.col('years_since_first_enrollment').alias('previous_years_since_first_enrollment'),
            pl.col('enrollment_source').alias('previous_enrollment_source'),
            pl.col('enrollment_method').alias('previous_enrollment_method'),
            pl.col('ever_opted_out').alias('previous_ever_opted_out'),
            pl.col('ever_unenrolled').alias('previous_ever_unenrolled')
        ])

        if current_year_summary.height == 0:
            # No events this year, carry forward previous state
            return previous.select([
                'employee_id',
                pl.lit(simulation_year).alias('simulation_year'),
                pl.col('previous_enrollment_date').alias('enrollment_date'),
                pl.col('previous_enrollment_status').alias('enrollment_status'),
                (pl.col('previous_years_since_first_enrollment') + 1).alias('years_since_first_enrollment'),
                pl.col('previous_enrollment_source').alias('enrollment_source'),
                pl.col('previous_enrollment_method').alias('enrollment_method'),
                pl.col('previous_ever_opted_out').alias('ever_opted_out'),
                pl.col('previous_ever_unenrolled').alias('ever_unenrolled'),
                pl.lit(0).alias('enrollment_events_this_year'),
                pl.lit(0).alias('enrollment_change_events_this_year'),
                # Additional columns to match dbt schema (E097 fix)
                pl.when(pl.col('previous_enrollment_status') == True)
                .then(pl.col('previous_enrollment_date'))
                .otherwise(None)
                .alias('effective_enrollment_date'),
                pl.when(pl.col('previous_enrollment_status') == True)
                .then(pl.lit(True))
                .otherwise(pl.lit(False))
                .alias('is_enrolled'),
                pl.lit(datetime.now()).alias('created_at'),
                pl.lit('default').alias('scenario_id'),
                pl.lit('VALID').alias('data_quality_flag')
            ])

        # Join previous state with current year events
        combined = previous.join(
            current_year_summary,
            on='employee_id',
            how='outer'
        ).with_columns([
            # Effective enrollment date (new events override previous)
            pl.when(pl.col('enrollment_event_date').is_not_null())
            .then(pl.col('enrollment_event_date'))
            .when(pl.col('previous_enrollment_date').is_not_null())
            .then(pl.col('previous_enrollment_date'))
            .otherwise(None)
            .alias('enrollment_date'),

            # Effective enrollment status
            pl.when(pl.col('has_enrollment_event_this_year').is_not_null())
            .then(pl.col('has_enrollment_event_this_year'))
            .when(pl.col('previous_enrollment_status').is_not_null())
            .then(pl.col('previous_enrollment_status'))
            .otherwise(pl.lit(False))
            .alias('enrollment_status'),

            # Years since first enrollment
            pl.when(pl.col('enrollment_event_date').is_not_null())
            .then(pl.lit(0))  # Reset counter for new enrollments
            .when(pl.col('previous_years_since_first_enrollment').is_not_null())
            .then(pl.col('previous_years_since_first_enrollment') + 1)
            .otherwise(None)
            .alias('years_since_first_enrollment'),

            # Enrollment source
            pl.when(pl.col('enrollment_event_date').is_not_null())
            .then(pl.lit(f'event_{simulation_year}'))
            .when(pl.col('previous_enrollment_source').is_not_null())
            .then(pl.col('previous_enrollment_source'))
            .otherwise(pl.lit('none'))
            .alias('enrollment_source'),

            # Enrollment method (preserve if no new event)
            pl.when(pl.col('enrollment_method_this_year').is_not_null())
            .then(pl.col('enrollment_method_this_year'))
            .when(pl.col('previous_enrollment_method').is_not_null())
            .then(pl.col('previous_enrollment_method'))
            .otherwise(None)
            .alias('enrollment_method'),

            # Ever opted out (cumulative)
            (pl.col('previous_ever_opted_out').fill_null(False) |
             pl.col('had_opt_out_this_year').fill_null(False))
            .alias('ever_opted_out'),

            # Ever unenrolled (enrolled then opted out)
            pl.when(
                pl.col('previous_enrollment_status') &
                pl.col('had_opt_out_this_year').fill_null(False)
            )
            .then(pl.lit(True))
            .otherwise(pl.col('previous_ever_unenrolled').fill_null(False))
            .alias('ever_unenrolled'),

            # Event counts
            pl.col('enrollment_events_count').fill_null(0).alias('enrollment_events_this_year'),
            pl.col('enrollment_change_events_count').fill_null(0).alias('enrollment_change_events_this_year')
        ])

        return combined.select([
            'employee_id',
            pl.lit(simulation_year).alias('simulation_year'),
            'enrollment_date',
            'enrollment_status',
            'years_since_first_enrollment',
            'enrollment_source',
            'enrollment_method',
            'ever_opted_out',
            'ever_unenrolled',
            'enrollment_events_this_year',
            'enrollment_change_events_this_year',
            # Additional columns to match dbt schema (E097 fix)
            pl.when(pl.col('enrollment_status') == True)
            .then(pl.col('enrollment_date'))
            .otherwise(None)
            .alias('effective_enrollment_date'),
            pl.when(pl.col('enrollment_status') == True)
            .then(pl.lit(True))
            .otherwise(pl.lit(False))
            .alias('is_enrolled'),
            pl.lit(datetime.now()).alias('created_at'),
            pl.lit('default').alias('scenario_id'),
            pl.when(pl.col('employee_id').is_null())
            .then(pl.lit('INVALID_EMPLOYEE_ID'))
            .when(pl.col('enrollment_status').is_null())
            .then(pl.lit('INVALID_ENROLLMENT_STATUS'))
            .otherwise(pl.lit('VALID'))
            .alias('data_quality_flag')
        ])


class DeferralRateBuilder:
    """
    Builds temporal deferral rate state with escalation tracking.

    Replaces: int_deferral_rate_state_accumulator.sql
    Performance Target: <500ms per builder

    Architecture:
        - Tracks deferral rates for enrolled employees
        - Handles escalation events (automatic and manual)
        - Maintains historical deferral rate changes
        - Supports baseline rates by age/income segment
    """

    def __init__(self, logger: logging.Logger):
        """Initialize deferral rate state builder."""
        self.logger = logger
        # Default baseline rates by segment
        self.default_rates = {
            ('young', 'low_income'): 0.03,
            ('young', 'moderate'): 0.04,
            ('young', 'high'): 0.05,
            ('young', 'executive'): 0.06,
            ('mid_career', 'low_income'): 0.04,
            ('mid_career', 'moderate'): 0.05,
            ('mid_career', 'high'): 0.06,
            ('mid_career', 'executive'): 0.08,
            ('senior', 'low_income'): 0.05,
            ('senior', 'moderate'): 0.06,
            ('senior', 'high'): 0.08,
            ('senior', 'executive'): 0.10,
            ('mature', 'low_income'): 0.06,
            ('mature', 'moderate'): 0.08,
            ('mature', 'high'): 0.10,
            ('mature', 'executive'): 0.12,
        }

    def build(
        self,
        simulation_year: int,
        events_df: pl.DataFrame,
        enrollment_state_df: pl.DataFrame,
        baseline_df: pl.DataFrame,
        previous_state_df: Optional[pl.DataFrame] = None
    ) -> pl.DataFrame:
        """
        Build deferral rate state for the simulation year.

        Args:
            simulation_year: Current simulation year
            events_df: All events for current year
            enrollment_state_df: Enrollment state (for enrolled employees list)
            baseline_df: Baseline workforce (for demographics)
            previous_state_df: Previous year's deferral state (Year 2+)

        Returns:
            DataFrame with deferral rate state for current year
        """
        start_time = time.time()

        # Get enrolled employees
        enrolled = enrollment_state_df.filter(
            pl.col('enrollment_status') == True
        ).select(['employee_id', 'enrollment_date'])

        if enrolled.height == 0:
            self.logger.warning(f"No enrolled employees for year {simulation_year}")
            return pl.DataFrame()

        # Get employee demographics for segmentation
        demographics = self._get_employee_demographics(baseline_df, enrolled)

        # Extract deferral-related events
        deferral_events = self._extract_deferral_events(events_df, simulation_year)

        # Build deferral state
        if previous_state_df is None:
            # Year 1: Use baseline rates + enrollment events
            deferral_state = self._build_year1_state(
                enrolled, demographics, deferral_events, simulation_year
            )
        else:
            # Year 2+: Use previous state + events
            deferral_state = self._build_subsequent_year_state(
                enrolled, demographics, deferral_events, previous_state_df, simulation_year
            )

        elapsed = time.time() - start_time
        self.logger.info(
            f"Built deferral rate state for {deferral_state.height} employees "
            f"in {elapsed:.3f}s"
        )

        return deferral_state

    def _get_employee_demographics(
        self, baseline_df: pl.DataFrame, enrolled_df: pl.DataFrame
    ) -> pl.DataFrame:
        """Get employee demographics for age/income segmentation."""
        # Join enrolled employees with baseline for demographics
        demographics = enrolled_df.join(
            baseline_df.select([
                'employee_id',
                'employee_gross_compensation',
                'employee_birth_date'
            ]),
            on='employee_id',
            how='left'
        )

        # Add age calculation (approximate)
        current_date = date.today()
        demographics = demographics.with_columns([
            # Calculate age
            pl.when(pl.col('employee_birth_date').is_not_null())
            .then(
                (pl.lit(current_date) - pl.col('employee_birth_date').cast(pl.Date))
                .dt.total_days() / 365.25
            )
            .otherwise(40.0)  # Default age
            .cast(pl.Int32)
            .alias('current_age'),

            # Ensure compensation exists
            pl.col('employee_gross_compensation').fill_null(75000.0).alias('compensation')
        ])

        # Add segmentation
        demographics = demographics.with_columns([
            # Age segment
            pl.when(pl.col('current_age') < 30).then(pl.lit('young'))
            .when(pl.col('current_age') < 45).then(pl.lit('mid_career'))
            .when(pl.col('current_age') < 55).then(pl.lit('senior'))
            .otherwise(pl.lit('mature'))
            .alias('age_segment'),

            # Income segment
            pl.when(pl.col('compensation') >= 250000).then(pl.lit('executive'))
            .when(pl.col('compensation') >= 150000).then(pl.lit('high'))
            .when(pl.col('compensation') >= 100000).then(pl.lit('moderate'))
            .otherwise(pl.lit('low_income'))
            .alias('income_segment')
        ])

        return demographics

    def _extract_deferral_events(
        self, events_df: pl.DataFrame, simulation_year: int
    ) -> pl.DataFrame:
        """Extract deferral-related events (enrollments and escalations)."""
        if events_df.height == 0:
            return pl.DataFrame()

        # Get enrollment events with deferral rates
        enrollment_events = events_df.filter(
            (pl.col('event_type').is_in(['enrollment', 'enrollment_change'])) &
            pl.col('employee_deferral_rate').is_not_null()
        ).select([
            'employee_id',
            'simulation_year',
            'effective_date',
            'employee_deferral_rate',
            'event_type',
            'event_details'
        ])

        # Get deferral escalation events
        escalation_events = events_df.filter(
            pl.col('event_type') == 'deferral_escalation'
        ).select([
            'employee_id',
            'simulation_year',
            'effective_date',
            'employee_deferral_rate',
            pl.lit('deferral_escalation').alias('event_type'),
            'event_details'
        ])

        # Combine events
        if enrollment_events.height > 0 and escalation_events.height > 0:
            all_events = pl.concat([enrollment_events, escalation_events], how='diagonal')
        elif enrollment_events.height > 0:
            all_events = enrollment_events
        elif escalation_events.height > 0:
            all_events = escalation_events
        else:
            return pl.DataFrame()

        return all_events.sort(['employee_id', 'effective_date'])

    def _build_year1_state(
        self,
        enrolled_df: pl.DataFrame,
        demographics_df: pl.DataFrame,
        deferral_events_df: pl.DataFrame,
        simulation_year: int
    ) -> pl.DataFrame:
        """Build Year 1 deferral state from baseline rates + events."""
        # Start with demographics and baseline rates
        base_state = demographics_df.with_columns([
            pl.struct(['age_segment', 'income_segment'])
            .map_elements(
                lambda x: self.default_rates.get((x['age_segment'], x['income_segment']), 0.06),
                return_dtype=pl.Float64
            )
            .alias('baseline_deferral_rate')
        ])

        # If no events, use baseline rates
        if deferral_events_df.height == 0:
            return base_state.select([
                'employee_id',
                pl.lit(simulation_year).alias('simulation_year'),
                pl.col('baseline_deferral_rate').alias('current_deferral_rate'),
                pl.lit(0).alias('escalation_count'),
                pl.lit(None, dtype=pl.Date).alias('last_escalation_date'),
                pl.lit(False).alias('had_escalation_this_year'),
                pl.col('age_segment'),
                pl.col('income_segment')
            ])

        # Get latest deferral rate from events for each employee
        latest_rates = deferral_events_df.sort('effective_date', descending=True).group_by('employee_id').agg([
            pl.col('employee_deferral_rate').first().alias('event_deferral_rate'),
            pl.col('effective_date').first().alias('last_event_date'),
            pl.col('event_type').filter(pl.col('event_type') == 'deferral_escalation').count().alias('escalation_count'),
            pl.col('effective_date').filter(pl.col('event_type') == 'deferral_escalation').max().alias('last_escalation_date')
        ])

        # Join with base state
        combined = base_state.join(latest_rates, on='employee_id', how='left')

        return combined.select([
            'employee_id',
            pl.lit(simulation_year).alias('simulation_year'),
            # Use event rate if available, otherwise baseline
            pl.coalesce(['event_deferral_rate', 'baseline_deferral_rate']).alias('current_deferral_rate'),
            pl.col('escalation_count').fill_null(0).alias('escalation_count'),
            'last_escalation_date',
            (pl.col('escalation_count').fill_null(0) > 0).alias('had_escalation_this_year'),
            'age_segment',
            'income_segment'
        ])

    def _build_subsequent_year_state(
        self,
        enrolled_df: pl.DataFrame,
        demographics_df: pl.DataFrame,
        deferral_events_df: pl.DataFrame,
        previous_state_df: pl.DataFrame,
        simulation_year: int
    ) -> pl.DataFrame:
        """Build Year 2+ deferral state from previous state + events."""
        # Get current year events (handle empty DataFrame)
        if deferral_events_df.height == 0:
            current_events = pl.DataFrame()
        else:
            current_events = deferral_events_df.filter(
                pl.col('simulation_year') == simulation_year
            )

        # If no events this year, carry forward previous state
        if current_events.height == 0:
            return previous_state_df.join(
                enrolled_df.select('employee_id'),
                on='employee_id',
                how='inner'  # Only keep enrolled employees
            ).with_columns([
                pl.lit(simulation_year).alias('simulation_year'),
                pl.lit(False).alias('had_escalation_this_year')
            ])

        # Get latest rates from current year events
        latest_rates = current_events.sort('effective_date', descending=True).group_by('employee_id').agg([
            pl.col('employee_deferral_rate').first().alias('new_deferral_rate'),
            pl.col('effective_date').first().alias('last_event_date'),
            pl.col('event_type').filter(pl.col('event_type') == 'deferral_escalation').count().alias('new_escalations'),
            pl.col('effective_date').filter(pl.col('event_type') == 'deferral_escalation').max().alias('latest_escalation_date')
        ])

        # Join previous state with demographics and current events
        combined = enrolled_df.join(
            previous_state_df.select([
                'employee_id',
                'current_deferral_rate',
                'escalation_count',
                'last_escalation_date'
            ]),
            on='employee_id',
            how='left'
        ).join(
            demographics_df.select(['employee_id', 'age_segment', 'income_segment']),
            on='employee_id',
            how='left'
        ).join(
            latest_rates,
            on='employee_id',
            how='left'
        )

        return combined.select([
            'employee_id',
            pl.lit(simulation_year).alias('simulation_year'),
            # Use new rate if available, otherwise carry forward previous
            pl.coalesce(['new_deferral_rate', 'current_deferral_rate']).alias('current_deferral_rate'),
            # Accumulate escalation count
            (pl.col('escalation_count').fill_null(0) + pl.col('new_escalations').fill_null(0)).alias('escalation_count'),
            # Use latest escalation date
            pl.coalesce(['latest_escalation_date', 'last_escalation_date']).alias('last_escalation_date'),
            # Had escalation this year
            (pl.col('new_escalations').fill_null(0) > 0).alias('had_escalation_this_year'),
            'age_segment',
            'income_segment'
        ])


class ContributionsCalculator:
    """
    Vectorized contribution calculations with match formulas.

    Replaces: int_employee_contributions.sql, int_employee_match_calculations.sql
    Performance Target: <500ms for 10k employees

    Features:
        - IRS 402(g) limit enforcement ($23,500 base / $31,000 catch-up for 2025)
        - Multiple match formulas (simple, tiered, stretch)
        - Prorated contributions for partial-year employees
        - Employer match calculations with eligibility filtering
    """

    # IRS 402(g) limits by year
    IRS_LIMITS = {
        2025: {'base': 23500, 'catch_up': 31000, 'catch_up_age': 50},
        2026: {'base': 24000, 'catch_up': 31500, 'catch_up_age': 50},
        2027: {'base': 24500, 'catch_up': 32000, 'catch_up_age': 50},
    }

    # Default match formulas
    DEFAULT_MATCH_FORMULAS = {
        'simple_match': {
            'type': 'simple',
            'match_rate': 0.50,
            'max_match_percentage': 0.03
        },
        'tiered_match': {
            'type': 'tiered',
            'tiers': [
                {'min': 0.00, 'max': 0.03, 'rate': 1.00},
                {'min': 0.03, 'max': 0.05, 'rate': 0.50}
            ],
            'max_match_percentage': 0.04
        },
        'stretch_match': {
            'type': 'tiered',
            'tiers': [
                {'min': 0.00, 'max': 0.12, 'rate': 0.25}
            ],
            'max_match_percentage': 0.03
        }
    }

    def __init__(self, logger: logging.Logger, match_formula: str = 'simple_match'):
        """Initialize contributions calculator."""
        self.logger = logger
        self.match_formula = match_formula
        if match_formula not in self.DEFAULT_MATCH_FORMULAS:
            self.logger.warning(f"Unknown match formula '{match_formula}', using simple_match")
            self.match_formula = 'simple_match'

    def calculate(
        self,
        simulation_year: int,
        enrollment_state_df: pl.DataFrame,
        deferral_state_df: pl.DataFrame,
        baseline_df: pl.DataFrame,
        events_df: pl.DataFrame
    ) -> pl.DataFrame:
        """
        Calculate employee contributions with IRS compliance and employer match.

        Args:
            simulation_year: Current simulation year
            enrollment_state_df: Enrollment state for current year
            deferral_state_df: Deferral rate state for current year
            baseline_df: Baseline workforce data
            events_df: Events for current year

        Returns:
            DataFrame with contribution calculations
        """
        start_time = time.time()

        # Get IRS limits for simulation year
        irs_limits = self.IRS_LIMITS.get(simulation_year, self.IRS_LIMITS[2025])

        # Get enrolled employees with deferral rates
        enrolled_with_rates = enrollment_state_df.filter(
            pl.col('enrollment_status') == True
        ).join(
            deferral_state_df.select(['employee_id', 'current_deferral_rate']),
            on='employee_id',
            how='left'
        )

        # Join with baseline for compensation
        contributions_base = enrolled_with_rates.join(
            baseline_df.select([
                'employee_id',
                'employee_gross_compensation',
                'employee_birth_date',
                pl.col('employee_hire_date').alias('hire_date')
            ]),
            on='employee_id',
            how='left'
        )

        # Add compensation and age calculations
        year_start = date(simulation_year, 1, 1)
        year_end = date(simulation_year, 12, 31)

        # Process termination events for proration
        termination_events = events_df.filter(
            (pl.col('event_type').str.to_lowercase() == 'termination') &
            (pl.col('simulation_year') == simulation_year)
        ).select([
            'employee_id',
            pl.col('effective_date').alias('termination_date')
        ]).group_by('employee_id').agg(
            pl.col('termination_date').max()
        )

        # Process hire events for proration
        hire_events = events_df.filter(
            (pl.col('event_type').str.to_lowercase() == 'hire') &
            (pl.col('simulation_year') == simulation_year)
        ).select([
            'employee_id',
            pl.col('effective_date').alias('event_hire_date'),
            pl.col('compensation_amount').alias('hire_compensation')
        ]).group_by('employee_id').agg([
            pl.col('event_hire_date').min(),
            pl.col('hire_compensation').last()
        ])

        # Add termination and hire info
        contributions_base = contributions_base.join(
            termination_events,
            on='employee_id',
            how='left'
        ).join(
            hire_events,
            on='employee_id',
            how='left'
        )

        # Calculate contributions
        contributions = contributions_base.with_columns([
            pl.lit(simulation_year).alias('simulation_year'),

            # Calculate current age
            pl.when(pl.col('employee_birth_date').is_not_null())
            .then(
                (pl.lit(year_end) - pl.col('employee_birth_date').cast(pl.Date)).dt.total_days() / 365.25
            )
            .otherwise(40.0)
            .cast(pl.Int32)
            .alias('current_age'),

            # Effective compensation (use hire compensation for new hires)
            pl.coalesce([
                pl.col('hire_compensation'),
                pl.col('employee_gross_compensation')
            ]).fill_null(75000.0).alias('effective_compensation'),

            # Employment start date for proration
            pl.coalesce([
                pl.col('event_hire_date').cast(pl.Date),
                pl.col('hire_date').cast(pl.Date),
                pl.lit(year_start)
            ]).alias('employment_start'),

            # Employment end date for proration
            pl.coalesce([
                pl.col('termination_date').cast(pl.Date),
                pl.lit(year_end)
            ]).alias('employment_end'),

            # Ensure deferral rate is valid
            pl.col('current_deferral_rate').fill_null(0.0).clip(0.0, 1.0).alias('deferral_rate')
        ])

        # Calculate prorated compensation and contributions
        contributions = contributions.with_columns([
            # Days employed in year
            pl.when(pl.col('employment_end') >= pl.col('employment_start'))
            .then(
                (pl.col('employment_end').cast(pl.Date) -
                 pl.col('employment_start').cast(pl.Date)).dt.total_days() + 1
            )
            .otherwise(0)
            .clip(0, 365)
            .alias('days_employed'),
        ]).with_columns([
            # Prorated compensation
            (pl.col('effective_compensation') * pl.col('days_employed') / 365.0)
            .round(2)
            .alias('prorated_annual_compensation'),
        ]).with_columns([
            # Requested contribution (before IRS limit)
            (pl.col('prorated_annual_compensation') * pl.col('deferral_rate'))
            .round(2)
            .alias('requested_contribution'),

            # IRS limit based on age
            pl.when(pl.col('current_age') >= irs_limits['catch_up_age'])
            .then(pl.lit(float(irs_limits['catch_up'])))
            .otherwise(pl.lit(float(irs_limits['base'])))
            .alias('applicable_irs_limit'),

            # Limit type
            pl.when(pl.col('current_age') >= irs_limits['catch_up_age'])
            .then(pl.lit('CATCH_UP'))
            .otherwise(pl.lit('BASE'))
            .alias('irs_limit_type')
        ]).with_columns([
            # IRS-compliant contribution (capped)
            pl.min_horizontal(['requested_contribution', 'applicable_irs_limit'])
            .round(2)
            .alias('annual_contribution_amount'),

            # Was limit applied?
            (pl.col('requested_contribution') > pl.col('applicable_irs_limit'))
            .alias('irs_limit_applied'),

            # Amount capped
            pl.max_horizontal([
                pl.lit(0.0),
                pl.col('requested_contribution') - pl.col('applicable_irs_limit')
            ]).round(2).alias('amount_capped_by_irs')
        ])

        # Calculate employer match
        contributions = self._calculate_employer_match(contributions, simulation_year)

        # Final output columns
        result = contributions.select([
            'employee_id',
            'simulation_year',
            'current_age',
            'effective_compensation',
            'prorated_annual_compensation',
            'deferral_rate',
            'annual_contribution_amount',
            'requested_contribution',
            'applicable_irs_limit',
            'irs_limit_applied',
            'amount_capped_by_irs',
            'irs_limit_type',
            'employer_match_amount',
            'match_formula_type',
            'days_employed',
            pl.when(pl.col('termination_date').is_not_null())
            .then(pl.lit('partial_year'))
            .otherwise(pl.lit('full_year'))
            .alias('contribution_duration'),
            pl.when(pl.col('irs_limit_applied'))
            .then(pl.lit('IRS_LIMITED'))
            .otherwise(pl.lit('NORMAL'))
            .alias('contribution_quality_flag')
        ])

        elapsed = time.time() - start_time
        self.logger.info(
            f"Calculated contributions for {result.height} employees "
            f"in {elapsed:.3f}s"
        )

        return result

    def _calculate_employer_match(
        self,
        contributions_df: pl.DataFrame,
        simulation_year: int
    ) -> pl.DataFrame:
        """Calculate employer match based on formula."""
        formula = self.DEFAULT_MATCH_FORMULAS[self.match_formula]

        if formula['type'] == 'simple':
            # Simple match: rate * min(deferral_rate, max_match_pct) * compensation
            match_rate = formula['match_rate']
            max_match_pct = formula['max_match_percentage']

            return contributions_df.with_columns([
                (
                    pl.col('prorated_annual_compensation') *
                    pl.min_horizontal([
                        pl.col('deferral_rate'),
                        pl.lit(max_match_pct)
                    ]) *
                    pl.lit(match_rate)
                ).round(2).alias('employer_match_amount'),
                pl.lit('simple').alias('match_formula_type')
            ])

        elif formula['type'] == 'tiered':
            # Tiered match: sum of tier contributions
            max_match_pct = formula['max_match_percentage']

            # Calculate match for each tier and sum
            match_exprs = []
            for tier in formula['tiers']:
                tier_contribution = (
                    pl.when(pl.col('deferral_rate') > tier['min'])
                    .then(
                        pl.min_horizontal([
                            pl.col('deferral_rate') - pl.lit(tier['min']),
                            pl.lit(tier['max'] - tier['min'])
                        ]) * pl.lit(tier['rate']) * pl.col('prorated_annual_compensation')
                    )
                    .otherwise(0.0)
                )
                match_exprs.append(tier_contribution)

            # Sum all tier contributions and apply max cap
            total_match = sum(match_exprs) if match_exprs else pl.lit(0.0)

            return contributions_df.with_columns([
                pl.min_horizontal([
                    total_match,
                    pl.col('prorated_annual_compensation') * pl.lit(max_match_pct)
                ]).round(2).alias('employer_match_amount'),
                pl.lit('tiered').alias('match_formula_type')
            ])

        else:
            # Default: no match
            return contributions_df.with_columns([
                pl.lit(0.0).alias('employer_match_amount'),
                pl.lit('none').alias('match_formula_type')
            ])


class SnapshotBuilder:
    """
    Final workforce snapshot generation with status classification.

    Replaces: fct_workforce_snapshot.sql
    Performance Target: <1s for 10k employees

    Features:
        - Employee status classification (continuous_active, new_hire_active, etc.)
        - Age and tenure band calculations
        - Prorated compensation calculations
        - Participation status tracking
        - Full year equivalent compensation
    """

    # Age band definitions
    AGE_BANDS = [
        (0, 25, '< 25'),
        (25, 35, '25-34'),
        (35, 45, '35-44'),
        (45, 55, '45-54'),
        (55, 65, '55-64'),
        (65, 200, '65+')
    ]

    # Tenure band definitions
    TENURE_BANDS = [
        (0, 2, '< 2'),
        (2, 5, '2-4'),
        (5, 10, '5-9'),
        (10, 20, '10-19'),
        (20, 100, '20+')
    ]

    def __init__(self, logger: logging.Logger):
        """Initialize snapshot builder."""
        self.logger = logger

    def build(
        self,
        simulation_year: int,
        baseline_df: pl.DataFrame,
        events_df: pl.DataFrame,
        enrollment_state_df: pl.DataFrame,
        deferral_state_df: pl.DataFrame,
        contributions_df: pl.DataFrame
    ) -> pl.DataFrame:
        """
        Build final workforce snapshot for the simulation year.

        Args:
            simulation_year: Current simulation year
            baseline_df: Baseline workforce data
            events_df: Events for current year
            enrollment_state_df: Enrollment state for current year
            deferral_state_df: Deferral rate state for current year
            contributions_df: Contribution calculations for current year

        Returns:
            DataFrame with complete workforce snapshot
        """
        start_time = time.time()

        year_start = date(simulation_year, 1, 1)
        year_end = date(simulation_year, 12, 31)

        # Extract key event types with defensive column handling
        # Some events may not have all columns depending on event source
        hire_filter = (
            (pl.col('event_type').str.to_lowercase() == 'hire') &
            (pl.col('simulation_year') == simulation_year)
        )

        hire_events_raw = events_df.filter(hire_filter)

        if hire_events_raw.height > 0:
            # Build select list based on available columns
            select_cols = ['employee_id', pl.col('effective_date').alias('hire_event_date')]

            if 'compensation_amount' in hire_events_raw.columns:
                select_cols.append(pl.col('compensation_amount').alias('hire_compensation'))
            else:
                select_cols.append(pl.lit(75000.0).alias('hire_compensation'))

            if 'employee_age' in hire_events_raw.columns:
                select_cols.append(pl.col('employee_age').alias('hire_age'))
            else:
                select_cols.append(pl.lit(30).alias('hire_age'))

            if 'level_id' in hire_events_raw.columns:
                select_cols.append(pl.col('level_id'))
            else:
                select_cols.append(pl.lit(1).alias('level_id'))

            hire_events = hire_events_raw.select(select_cols).group_by('employee_id').agg([
                pl.col('hire_event_date').min(),
                pl.col('hire_compensation').last(),
                pl.col('hire_age').last(),
                pl.col('level_id').last()
            ])
        else:
            # Empty DataFrame with expected schema
            hire_events = pl.DataFrame({
                'employee_id': pl.Series([], dtype=pl.Utf8),
                'hire_event_date': pl.Series([], dtype=pl.Date),
                'hire_compensation': pl.Series([], dtype=pl.Float64),
                'hire_age': pl.Series([], dtype=pl.Int64),
                'level_id': pl.Series([], dtype=pl.Int64)
            })

        termination_filter = (
            (pl.col('event_type').str.to_lowercase() == 'termination') &
            (pl.col('simulation_year') == simulation_year)
        )
        termination_events_raw = events_df.filter(termination_filter)

        if termination_events_raw.height > 0:
            select_cols = ['employee_id', pl.col('effective_date').alias('termination_date')]
            if 'event_details' in termination_events_raw.columns:
                select_cols.append(pl.col('event_details').alias('termination_reason'))
            else:
                select_cols.append(pl.lit(None, dtype=pl.Utf8).alias('termination_reason'))

            termination_events = termination_events_raw.select(select_cols).group_by('employee_id').agg([
                pl.col('termination_date').max(),
                pl.col('termination_reason').last()
            ])
        else:
            termination_events = pl.DataFrame({
                'employee_id': pl.Series([], dtype=pl.Utf8),
                'termination_date': pl.Series([], dtype=pl.Date),
                'termination_reason': pl.Series([], dtype=pl.Utf8)
            })

        # Compensation events (merit/promotion)
        comp_filter = (
            (pl.col('event_type').str.to_lowercase().is_in(['raise', 'promotion', 'merit'])) &
            (pl.col('simulation_year') == simulation_year)
        )
        compensation_events_raw = events_df.filter(comp_filter)

        if compensation_events_raw.height > 0:
            select_cols = ['employee_id', 'event_type']
            if 'compensation_amount' in compensation_events_raw.columns:
                select_cols.append(pl.col('compensation_amount').alias('new_compensation'))
            else:
                select_cols.append(pl.lit(None, dtype=pl.Float64).alias('new_compensation'))

            if 'level_id' in compensation_events_raw.columns:
                select_cols.append(pl.col('level_id').alias('new_level_id'))
            else:
                select_cols.append(pl.lit(None, dtype=pl.Int64).alias('new_level_id'))

            compensation_events = compensation_events_raw.select(select_cols).group_by('employee_id').agg([
                pl.col('new_compensation').last(),
                pl.col('new_level_id').last(),
                pl.col('event_type').last()
            ])
        else:
            compensation_events = pl.DataFrame({
                'employee_id': pl.Series([], dtype=pl.Utf8),
                'new_compensation': pl.Series([], dtype=pl.Float64),
                'new_level_id': pl.Series([], dtype=pl.Int64),
                'event_type': pl.Series([], dtype=pl.Utf8)
            })

        # Start with baseline workforce
        snapshot = baseline_df.select([
            'employee_id',
            pl.col('employee_ssn').fill_null('').alias('employee_ssn'),
            'employee_birth_date',
            'employee_hire_date',
            'employee_gross_compensation',
            pl.col('employee_deferral_rate').fill_null(0.0),
            pl.col('employee_enrollment_date').alias('baseline_enrollment_date'),
            pl.col('active').fill_null(True)
        ]).with_columns([
            pl.lit(simulation_year).alias('simulation_year'),
            pl.lit('baseline').alias('record_source')
        ])

        # Add new hires from events
        new_hires = hire_events.select([
            'employee_id',
            pl.lit('').alias('employee_ssn'),
            pl.lit(None, dtype=pl.Date).alias('employee_birth_date'),
            pl.col('hire_event_date').cast(pl.Date).alias('employee_hire_date'),
            pl.col('hire_compensation').alias('employee_gross_compensation'),
            pl.lit(0.0).alias('employee_deferral_rate'),
            pl.lit(None, dtype=pl.Date).alias('baseline_enrollment_date'),
            pl.lit(True).alias('active'),
            pl.lit(simulation_year).alias('simulation_year'),
            pl.lit('new_hire').alias('record_source')
        ])

        # Combine baseline and new hires (prioritize new hire records)
        combined = pl.concat([snapshot, new_hires], how='diagonal')

        # Deduplicate (new hires take precedence)
        combined = combined.with_columns([
            pl.when(pl.col('record_source') == 'new_hire')
            .then(1)
            .otherwise(2)
            .alias('priority')
        ]).sort(['employee_id', 'priority']).group_by('employee_id').agg([
            pl.col('employee_ssn').first(),
            pl.col('employee_birth_date').first(),
            pl.col('employee_hire_date').first(),
            pl.col('employee_gross_compensation').first(),
            pl.col('employee_deferral_rate').first(),
            pl.col('baseline_enrollment_date').first(),
            pl.col('active').first(),
            pl.col('simulation_year').first(),
            pl.col('record_source').first()
        ])

        # Join all event data
        snapshot = combined.join(
            termination_events,
            on='employee_id',
            how='left'
        ).join(
            compensation_events,
            on='employee_id',
            how='left'
        ).join(
            enrollment_state_df.select([
                'employee_id',
                'enrollment_date',
                'enrollment_status',
                'enrollment_method',
                'ever_opted_out'
            ]),
            on='employee_id',
            how='left'
        ).join(
            deferral_state_df.select([
                'employee_id',
                'current_deferral_rate',
                'escalation_count',
                'had_escalation_this_year'
            ]),
            on='employee_id',
            how='left'
        ).join(
            contributions_df.select([
                'employee_id',
                'annual_contribution_amount',
                'prorated_annual_compensation',
                'employer_match_amount',
                'irs_limit_applied',
                'contribution_quality_flag'
            ]),
            on='employee_id',
            how='left'
        )

        # Calculate derived fields (with defensive column handling)
        derived_cols = [
            # Employment status
            pl.when(pl.col('termination_date').is_not_null())
            .then(pl.lit('terminated'))
            .otherwise(pl.lit('active'))
            .alias('employment_status'),

            # Current compensation (apply merit/promotion if exists)
            pl.coalesce([
                pl.col('new_compensation'),
                pl.col('employee_gross_compensation')
            ]).fill_null(75000.0).alias('current_compensation'),

            # Calculate current age
            pl.when(pl.col('employee_birth_date').is_not_null())
            .then(
                (pl.lit(year_end) - pl.col('employee_birth_date').cast(pl.Date)).dt.total_days() / 365.25
            )
            .otherwise(40.0)
            .cast(pl.Int32)
            .alias('current_age'),

            # Calculate tenure
            pl.when(pl.col('employee_hire_date').is_not_null())
            .then(
                (pl.lit(year_end) - pl.col('employee_hire_date').cast(pl.Date)).dt.total_days() / 365.25
            )
            .otherwise(5.0)
            .cast(pl.Int32)
            .alias('current_tenure'),

            # Is new hire this year?
            (
                pl.col('record_source') == 'new_hire'
            ).alias('is_new_hire'),

            # Enrollment date (from enrollment state or baseline)
            pl.coalesce([
                pl.col('enrollment_date'),
                pl.col('baseline_enrollment_date')
            ]).alias('employee_enrollment_date'),

            # Is enrolled flag
            pl.coalesce([
                pl.col('enrollment_status'),
                pl.col('baseline_enrollment_date').is_not_null()
            ]).fill_null(False).alias('is_enrolled_flag'),

            # Effective deferral rate
            pl.coalesce([
                pl.col('current_deferral_rate'),
                pl.col('employee_deferral_rate')
            ]).fill_null(0.0).alias('effective_annual_deferral_rate')
        ]

        # Handle level_id defensively (may not exist in all sources)
        if 'level_id' in snapshot.columns and 'new_level_id' in snapshot.columns:
            derived_cols.append(
                pl.coalesce([pl.col('new_level_id'), pl.col('level_id')])
                .fill_null(1).cast(pl.Int32).alias('level_id')
            )
        elif 'new_level_id' in snapshot.columns:
            derived_cols.append(
                pl.col('new_level_id').fill_null(1).cast(pl.Int32).alias('level_id')
            )
        elif 'level_id' in snapshot.columns:
            derived_cols.append(
                pl.col('level_id').fill_null(1).cast(pl.Int32).alias('level_id')
            )
        else:
            derived_cols.append(pl.lit(1).alias('level_id'))

        snapshot = snapshot.with_columns(derived_cols)

        # Add detailed status code
        snapshot = snapshot.with_columns([
            pl.when(
                (pl.col('is_new_hire')) & (pl.col('employment_status') == 'active')
            ).then(pl.lit('new_hire_active'))
            .when(
                (pl.col('is_new_hire')) & (pl.col('employment_status') == 'terminated')
            ).then(pl.lit('new_hire_termination'))
            .when(
                (pl.col('employment_status') == 'active') &
                (pl.col('is_new_hire') == False)
            ).then(pl.lit('continuous_active'))
            .when(
                (pl.col('employment_status') == 'terminated') &
                (pl.col('is_new_hire') == False)
            ).then(pl.lit('experienced_termination'))
            .otherwise(pl.lit('continuous_active'))
            .alias('detailed_status_code')
        ])

        # Add age bands
        age_band_expr = pl.lit('65+')  # Default
        for min_age, max_age, band_label in reversed(self.AGE_BANDS):
            age_band_expr = pl.when(
                (pl.col('current_age') >= min_age) & (pl.col('current_age') < max_age)
            ).then(pl.lit(band_label)).otherwise(age_band_expr)

        # Add tenure bands
        tenure_band_expr = pl.lit('20+')  # Default
        for min_tenure, max_tenure, band_label in reversed(self.TENURE_BANDS):
            tenure_band_expr = pl.when(
                (pl.col('current_tenure') >= min_tenure) & (pl.col('current_tenure') < max_tenure)
            ).then(pl.lit(band_label)).otherwise(tenure_band_expr)

        snapshot = snapshot.with_columns([
            age_band_expr.alias('age_band'),
            tenure_band_expr.alias('tenure_band')
        ])

        # Add participation status (with defensive handling for missing columns)
        participation_cols = [
            pl.when(pl.col('effective_annual_deferral_rate') > 0)
            .then(pl.lit('participating'))
            .otherwise(pl.lit('not_participating'))
            .alias('participation_status')
        ]

        # Handle enrollment_method defensively
        if 'enrollment_method' in snapshot.columns:
            participation_cols.append(
                pl.when(pl.col('effective_annual_deferral_rate') > 0)
                .then(
                    pl.when(pl.col('enrollment_method') == 'auto')
                    .then(pl.lit('participating - auto enrollment'))
                    .when(pl.col('enrollment_method') == 'voluntary')
                    .then(pl.lit('participating - voluntary enrollment'))
                    .otherwise(pl.lit('participating - census enrollment'))
                )
                .when(pl.col('ever_opted_out').fill_null(False) if 'ever_opted_out' in snapshot.columns else pl.lit(False))
                .then(pl.lit('not_participating - opted out of AE'))
                .otherwise(pl.lit('not_participating - not auto enrolled'))
                .alias('participation_status_detail')
            )
        else:
            participation_cols.append(
                pl.when(pl.col('effective_annual_deferral_rate') > 0)
                .then(pl.lit('participating - census enrollment'))
                .otherwise(pl.lit('not_participating - not auto enrolled'))
                .alias('participation_status_detail')
            )

        snapshot = snapshot.with_columns(participation_cols)

        # Add employer contribution calculations (with defensive handling)
        employer_cols = [pl.lit(0.0).alias('employer_core_amount')]

        if 'employer_match_amount' in snapshot.columns:
            employer_cols.append(pl.col('employer_match_amount').fill_null(0.0).alias('employer_match_amount'))
        else:
            employer_cols.append(pl.lit(0.0).alias('employer_match_amount'))

        snapshot = snapshot.with_columns(employer_cols).with_columns([
            (pl.col('employer_match_amount') + pl.col('employer_core_amount'))
            .alias('total_employer_contributions')
        ])

        # Calculate full year equivalent compensation (with defensive handling)
        if 'new_compensation' in snapshot.columns:
            snapshot = snapshot.with_columns([
                pl.when(pl.col('new_compensation').is_not_null())
                .then(pl.col('new_compensation'))
                .when(pl.col('is_new_hire'))
                .then(pl.col('current_compensation'))
                .otherwise(pl.col('current_compensation'))
                .alias('full_year_equivalent_compensation')
            ])
        else:
            snapshot = snapshot.with_columns([
                pl.col('current_compensation').alias('full_year_equivalent_compensation')
            ])

        # Add quality flags and timestamps
        snapshot = snapshot.with_columns([
            pl.when(pl.col('current_compensation') > 10000000)
            .then(pl.lit('CRITICAL_OVER_10M'))
            .when(pl.col('current_compensation') > 5000000)
            .then(pl.lit('SEVERE_OVER_5M'))
            .when(pl.col('current_compensation') > 2000000)
            .then(pl.lit('WARNING_OVER_2M'))
            .when((pl.col('current_compensation') < 10000) & (pl.col('employment_status') == 'active'))
            .then(pl.lit('WARNING_UNDER_10K'))
            .otherwise(pl.lit('NORMAL'))
            .alias('compensation_quality_flag'),

            pl.lit(datetime.now()).alias('snapshot_created_at')
        ])

        # Build select columns dynamically based on what's available
        select_cols = [
            'employee_id',
            'employee_ssn',
            'employee_birth_date',
            pl.col('employee_hire_date').cast(pl.Date),
            'current_compensation',
            # prorated_annual_compensation - use current_compensation if not available
            (pl.col('prorated_annual_compensation').fill_null(pl.col('current_compensation'))
             if 'prorated_annual_compensation' in snapshot.columns
             else pl.col('current_compensation')).alias('prorated_annual_compensation'),
            'full_year_equivalent_compensation',
            'current_age',
            'current_tenure',
            'level_id',
            'age_band',
            'tenure_band',
            'employment_status',
            'termination_date',
            (pl.col('termination_reason') if 'termination_reason' in snapshot.columns
             else pl.lit(None, dtype=pl.Utf8)).alias('termination_reason'),
            'detailed_status_code',
            'simulation_year',
            pl.lit(None, dtype=pl.Date).alias('employee_eligibility_date'),
            pl.lit(0).alias('waiting_period_days'),
            pl.lit('eligible').alias('current_eligibility_status'),
            'employee_enrollment_date',
            'is_enrolled_flag',
            'effective_annual_deferral_rate',
            'participation_status',
            'participation_status_detail',
            # escalation fields - defensive handling
            (pl.col('escalation_count').fill_null(0) if 'escalation_count' in snapshot.columns
             else pl.lit(0)).alias('total_deferral_escalations'),
            pl.lit(None, dtype=pl.Date).alias('last_escalation_date'),
            (pl.col('had_escalation_this_year').fill_null(False) if 'had_escalation_this_year' in snapshot.columns
             else pl.lit(False)).alias('has_deferral_escalations'),
            (pl.col('employee_deferral_rate').fill_null(0.0) if 'employee_deferral_rate' in snapshot.columns
             else pl.lit(0.0)).alias('original_deferral_rate'),
            pl.lit(0.0).alias('total_escalation_amount'),
            # contribution fields - defensive handling
            (pl.col('annual_contribution_amount').fill_null(0.0) if 'annual_contribution_amount' in snapshot.columns
             else pl.lit(0.0)).alias('prorated_annual_contributions'),
            ((pl.col('annual_contribution_amount').fill_null(0.0) * 0.85) if 'annual_contribution_amount' in snapshot.columns
             else pl.lit(0.0)).alias('pre_tax_contributions'),
            ((pl.col('annual_contribution_amount').fill_null(0.0) * 0.15) if 'annual_contribution_amount' in snapshot.columns
             else pl.lit(0.0)).alias('roth_contributions'),
            (pl.col('annual_contribution_amount').fill_null(0.0) if 'annual_contribution_amount' in snapshot.columns
             else pl.lit(0.0)).alias('ytd_contributions'),
            (pl.col('irs_limit_applied').fill_null(False) if 'irs_limit_applied' in snapshot.columns
             else pl.lit(False)).alias('irs_limit_reached'),
            pl.col('effective_annual_deferral_rate').alias('effective_annual_deferral_rate_2'),
            pl.col('current_compensation').alias('total_contribution_base_compensation'),
            pl.lit(None, dtype=pl.Date).alias('first_contribution_date'),
            pl.lit(None, dtype=pl.Date).alias('last_contribution_date'),
            (pl.col('contribution_quality_flag').fill_null('NORMAL') if 'contribution_quality_flag' in snapshot.columns
             else pl.lit('NORMAL')).alias('contribution_quality_flag'),
            'compensation_quality_flag',
            'employer_match_amount',
            'employer_core_amount',
            'total_employer_contributions',
            pl.lit(0).alias('annual_hours_worked'),
            'snapshot_created_at'
        ]

        result = snapshot.select(select_cols)

        elapsed = time.time() - start_time
        self.logger.info(
            f"Built snapshot for {result.height} employees "
            f"in {elapsed:.3f}s"
        )

        return result


def main():
    """CLI entry point for testing Polars state accumulation."""
    import argparse

    parser = argparse.ArgumentParser(
        description="Polars State Accumulation Pipeline - High-Performance State Processing"
    )

    parser.add_argument('--year', type=int, required=True,
                       help='Simulation year to process')
    parser.add_argument('--scenario', default='default',
                       help='Scenario ID (default: default)')
    parser.add_argument('--plan', default='default',
                       help='Plan design ID (default: default)')
    parser.add_argument('--events-path', type=Path,
                       help='Path to Parquet events directory')
    parser.add_argument('--validate', action='store_true',
                       help='Validate against dbt output')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='Enable verbose logging')

    args = parser.parse_args()

    # Setup logging
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    logger = logging.getLogger(__name__)

    try:
        # Create configuration
        config = StateAccumulatorConfig(
            simulation_year=args.year,
            scenario_id=args.scenario,
            plan_design_id=args.plan,
            events_path=args.events_path,
            enable_validation=args.validate
        )

        logger.info(f"Starting Polars state accumulation with config: {config}")

        # Build state
        engine = StateAccumulatorEngine(config)
        state_data = engine.build_state()

        print(f"\n✅ State accumulation complete!")
        print(f"📊 Employees processed: {engine.stats['employees_processed']:,}")
        print(f"📊 Events processed: {engine.stats['events_processed']:,}")
        print(f"⏱️  Total time: {engine.stats['total_processing_time']:.2f}s")

        # Validate if requested
        if args.validate:
            print(f"\n🔍 Validation enabled - comparing against dbt output...")
            # TODO: Implement validation logic in S076-05

    except Exception as e:
        logger.error(f"State accumulation failed: {e}", exc_info=True)
        print(f"\n❌ State accumulation failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
