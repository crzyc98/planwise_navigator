#!/usr/bin/env python3
"""
E076 Polars State Accumulation Pipeline - Maximum Performance State Processing

High-performance alternative to dbt-based state accumulation using Polars for
vectorized operations. Achieves 60-75% overall runtime improvement by replacing
the 70% bottleneck (state accumulation) with in-memory columnar processing.

This module provides:
- StateAccumulatorEngine: Core engine for state transformations
- EnrollmentStateBuilder: Temporal enrollment state tracking
- DeferralRateBuilder: Temporal deferral rate state tracking
- ContributionsCalculator: Vectorized contribution calculations
- SnapshotBuilder: Final workforce snapshot generation

Architecture:
    Events (Parquet) â†’ State Accumulators â†’ Contributions â†’ Snapshot â†’ DuckDB

Performance Target:
    State Accumulation: 20-25s â†’ 2-5s (80-90% reduction)
    Total Runtime: 236s â†’ 60-90s (60-75% improvement)
"""

import os
import sys
import json
import time
import logging
import polars as pl
import duckdb
from pathlib import Path
from datetime import date, datetime, timedelta
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Union

# Set Polars to use maximum threads for performance
os.environ.setdefault('POLARS_MAX_THREADS', '16')

# Import project modules
sys.path.insert(0, str(Path(__file__).parent.parent))
from navigator_orchestrator.config import load_simulation_config, get_database_path

# Module-level logger
logger = logging.getLogger(__name__)


@dataclass
class StateAccumulatorConfig:
    """Configuration for Polars state accumulation."""
    simulation_year: int
    scenario_id: str = "default"
    plan_design_id: str = "default"
    database_path: Optional[Path] = None  # If None, uses get_database_path()
    events_path: Optional[Path] = None  # Path to Parquet events directory
    output_path: Optional[Path] = None  # Output path for state data

    # Performance optimization settings
    enable_validation: bool = True  # Validate against dbt output
    enable_profiling: bool = False
    lazy_evaluation: bool = True
    streaming: bool = True

    def __post_init__(self):
        """Validate configuration after initialization."""
        if self.simulation_year < 2025:
            raise ValueError(f"simulation_year must be >= 2025, got {self.simulation_year}")


class StateAccumulatorEngine:
    """
    Core engine for Polars-based state accumulation.

    Replaces dbt state accumulation with vectorized in-memory operations
    for 80-90% performance improvement on the bottleneck stage.

    Architecture:
        1. Load events from Parquet (already generated by Polars or SQL)
        2. Build temporal state accumulators (enrollment, deferral rate)
        3. Calculate contributions using vectorized operations
        4. Generate final workforce snapshot
        5. Write results to DuckDB for dbt consumption
    """

    def __init__(self, config: StateAccumulatorConfig):
        """Initialize state accumulator engine with configuration."""
        self.config = config
        self.logger = self._setup_logging()

        # Performance monitoring
        self.start_time = time.time()
        self.stats = {
            'total_processing_time': 0.0,
            'enrollment_state_time': 0.0,
            'deferral_state_time': 0.0,
            'contributions_time': 0.0,
            'snapshot_time': 0.0,
            'employees_processed': 0,
            'events_processed': 0
        }

        # Resolve database path
        self.db_path = self.config.database_path if self.config.database_path else get_database_path()

        self.logger.info(f"Initialized StateAccumulatorEngine for year {config.simulation_year}")
        self.logger.info(f"Database path: {self.db_path}")

    def _setup_logging(self) -> logging.Logger:
        """Setup logging with appropriate level and formatting."""
        logger = logging.getLogger(__name__)

        # Don't add handlers if already configured
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)

        return logger

    def _load_events(self) -> pl.DataFrame:
        """
        Load events for current simulation year from Parquet or DuckDB.

        Tries multiple sources in order of preference:
        1. Parquet files (fastest - from Polars event generation)
        2. DuckDB fct_yearly_events table (fallback for SQL-generated events)

        Returns:
            Polars DataFrame with events for the current simulation year
        """
        # Try Parquet first (fastest path for Polars-generated events)
        if self.config.events_path:
            parquet_path = self.config.events_path / f"simulation_year={self.config.simulation_year}"
            if parquet_path.exists():
                self.logger.info(f"Loading events from Parquet: {parquet_path}")
                events_df = pl.read_parquet(parquet_path / f"events_{self.config.simulation_year}.parquet")
                self.stats['events_processed'] = events_df.height
                return events_df

        # Fallback to DuckDB
        self.logger.info(f"Loading events from DuckDB: {self.db_path}")
        if not self.db_path.exists():
            raise FileNotFoundError(f"Database not found: {self.db_path}")

        conn = duckdb.connect(str(self.db_path), read_only=True)
        query = f"""
            SELECT *
            FROM fct_yearly_events
            WHERE simulation_year = {self.config.simulation_year}
              AND scenario_id = '{self.config.scenario_id}'
        """
        events_df = conn.execute(query).pl()
        conn.close()

        self.stats['events_processed'] = events_df.height
        self.logger.info(f"Loaded {events_df.height} events from DuckDB")
        return events_df

    def _load_baseline_workforce(self) -> pl.DataFrame:
        """
        Load baseline workforce for Year 1 or previous year snapshot for Year 2+.

        Returns:
            Polars DataFrame with baseline workforce data
        """
        conn = duckdb.connect(str(self.db_path), read_only=True)

        # Year 1: Load from staging
        if self.config.simulation_year == 2025:  # TODO: Make this configurable
            self.logger.info("Year 1: Loading baseline workforce from stg_census_data")
            query = """
                SELECT
                    employee_id,
                    employee_ssn,
                    employee_birth_date,
                    employee_hire_date,
                    employee_gross_compensation,
                    employee_deferral_rate,
                    current_eligibility_status,
                    employee_enrollment_date,
                    active
                FROM stg_census_data
                WHERE active = true
            """
        else:
            # Year 2+: Load from previous year's snapshot
            prev_year = self.config.simulation_year - 1
            self.logger.info(f"Year {self.config.simulation_year}: Loading from Year {prev_year} snapshot")
            query = f"""
                SELECT
                    employee_id,
                    employee_ssn,
                    employee_birth_date,
                    employee_hire_date,
                    current_compensation as employee_gross_compensation,
                    effective_annual_deferral_rate as employee_deferral_rate,
                    current_eligibility_status,
                    employee_enrollment_date,
                    CASE WHEN employment_status = 'active' THEN true ELSE false END as active
                FROM fct_workforce_snapshot
                WHERE simulation_year = {prev_year}
                  AND scenario_id = '{self.config.scenario_id}'
                  AND employment_status = 'active'
            """

        baseline_df = conn.execute(query).pl()
        conn.close()

        self.stats['employees_processed'] = baseline_df.height
        self.logger.info(f"Loaded {baseline_df.height} baseline employees")
        return baseline_df

    def build_state(self) -> Dict[str, pl.DataFrame]:
        """
        Build all state accumulators for the current simulation year.

        Orchestrates the entire state accumulation pipeline:
        1. Load events and baseline workforce
        2. Build enrollment state
        3. Build deferral rate state
        4. Calculate contributions
        5. Generate workforce snapshot

        Returns:
            Dictionary containing all state DataFrames
        """
        total_start = time.time()
        self.logger.info(f"Building state for year {self.config.simulation_year}...")

        # Load input data
        events_df = self._load_events()
        baseline_df = self._load_baseline_workforce()

        # Build state accumulators (to be implemented in S076-02)
        # enrollment_state = self._build_enrollment_state(events_df, baseline_df)
        # deferral_state = self._build_deferral_state(events_df, baseline_df)

        # Calculate contributions (to be implemented in S076-03)
        # contributions = self._calculate_contributions(baseline_df, enrollment_state, deferral_state)

        # Generate snapshot (to be implemented in S076-04)
        # snapshot = self._build_snapshot(baseline_df, events_df, enrollment_state, deferral_state, contributions)

        total_time = time.time() - total_start
        self.stats['total_processing_time'] = total_time

        self.logger.info(f"State accumulation complete in {total_time:.2f}s")
        self.logger.info(f"Processed {self.stats['employees_processed']} employees, {self.stats['events_processed']} events")

        return {
            'events': events_df,
            'baseline': baseline_df,
            # 'enrollment_state': enrollment_state,
            # 'deferral_state': deferral_state,
            # 'contributions': contributions,
            # 'snapshot': snapshot
        }

    def write_to_database(self, state_data: Dict[str, pl.DataFrame]) -> None:
        """
        Write state data to DuckDB for dbt consumption.

        Args:
            state_data: Dictionary of state DataFrames to write
        """
        conn = duckdb.connect(str(self.db_path))

        # Write each state table (to be implemented)
        # TODO: Implement table writes

        conn.close()
        self.logger.info("State data written to database")


class EnrollmentStateBuilder:
    """
    Builds temporal enrollment state across simulation years.

    Replaces: int_enrollment_state_accumulator.sql
    Performance Target: <500ms per builder

    To be implemented in S076-02.
    """
    pass


class DeferralRateBuilder:
    """
    Builds temporal deferral rate state with escalation tracking.

    Replaces: int_deferral_rate_state_accumulator.sql
    Performance Target: <500ms per builder

    To be implemented in S076-02.
    """
    pass


class ContributionsCalculator:
    """
    Vectorized contribution calculations with match formulas.

    Replaces: int_employee_contributions_by_year.sql
    Performance Target: <500ms for 10k employees

    To be implemented in S076-03.
    """
    pass


class SnapshotBuilder:
    """
    Final workforce snapshot generation with status classification.

    Replaces: fct_workforce_snapshot.sql
    Performance Target: <1s for 10k employees

    To be implemented in S076-04.
    """
    pass


def main():
    """CLI entry point for testing Polars state accumulation."""
    import argparse

    parser = argparse.ArgumentParser(
        description="Polars State Accumulation Pipeline - High-Performance State Processing"
    )

    parser.add_argument('--year', type=int, required=True,
                       help='Simulation year to process')
    parser.add_argument('--scenario', default='default',
                       help='Scenario ID (default: default)')
    parser.add_argument('--plan', default='default',
                       help='Plan design ID (default: default)')
    parser.add_argument('--events-path', type=Path,
                       help='Path to Parquet events directory')
    parser.add_argument('--validate', action='store_true',
                       help='Validate against dbt output')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='Enable verbose logging')

    args = parser.parse_args()

    # Setup logging
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    logger = logging.getLogger(__name__)

    try:
        # Create configuration
        config = StateAccumulatorConfig(
            simulation_year=args.year,
            scenario_id=args.scenario,
            plan_design_id=args.plan,
            events_path=args.events_path,
            enable_validation=args.validate
        )

        logger.info(f"Starting Polars state accumulation with config: {config}")

        # Build state
        engine = StateAccumulatorEngine(config)
        state_data = engine.build_state()

        print(f"\nâœ… State accumulation complete!")
        print(f"ðŸ“Š Employees processed: {engine.stats['employees_processed']:,}")
        print(f"ðŸ“Š Events processed: {engine.stats['events_processed']:,}")
        print(f"â±ï¸  Total time: {engine.stats['total_processing_time']:.2f}s")

        # Validate if requested
        if args.validate:
            print(f"\nðŸ” Validation enabled - comparing against dbt output...")
            # TODO: Implement validation logic in S076-05

    except Exception as e:
        logger.error(f"State accumulation failed: {e}", exc_info=True)
        print(f"\nâŒ State accumulation failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
