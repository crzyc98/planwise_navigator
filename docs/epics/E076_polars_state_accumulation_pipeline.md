# Epic E076: Polars State Accumulation Pipeline

**Status**: üìã PLANNED (0% - epic not yet started)
**Priority**: üéØ MEDIUM - Performance optimization opportunity
**Estimated Effort**: 2-3 weeks (21-34 story points)
**Target Performance**: 70-80% reduction in state accumulation time
**Owner**: Engineering Team

---

## Executive Summary

Replace dbt-based state accumulation with a Polars-optimized pipeline to unlock dramatic performance improvements. Current performance analysis shows that Polars event generation achieves **26,000-28,000 events/second** but only provides a **4-6% overall improvement** because **70-80% of runtime** is spent in dbt state accumulation (20-25s per year). By extending the Polars pipeline to handle state accumulation, we can achieve the theoretical **10-100√ó performance gains** demonstrated in E068G.

**Current Bottleneck**:
- Event Generation: 0.1s (Polars) vs 0.3-0.5s (SQL) ‚Üí **Negligible difference**
- State Accumulation: 20-25s (dbt) ‚Üí **70-80% of total runtime**
- Validation: 2s (dbt)
- Total: 236s (Polars) vs 248s (SQL) ‚Üí **Only 4.6% improvement**

**Expected Impact After E076**:
- State Accumulation: 20-25s ‚Üí **2-5s** (80-90% reduction)
- Total Runtime: 236s ‚Üí **60-90s** (60-75% improvement)
- Event Processing: **50,000-100,000 events/second** end-to-end

---

## Problem Statement

### Current Performance Analysis

**2-Year Simulation Breakdown (2025-2026)**:

| Stage | Polars Mode | SQL Mode | Bottleneck? |
|-------|------------|----------|-------------|
| Initialization | 6.3s | 6.3s | ‚ùå No (same) |
| Foundation | 3.1s | 3.1s | ‚ùå No (same) |
| **Event Generation** | 0.1s | 0.5s | ‚ùå No (already optimized) |
| **State Accumulation** | 22s | 23s | ‚úÖ **YES - 70% of runtime** |
| Validation | 2s | 2s | ‚ùå No (same) |
| **Total** | **107s** | **113s** | - |

### Why Polars Event Generation Doesn't Help

The current Polars implementation (E068G) only optimizes **event generation**:

```
SQL Mode:  [Init 6s][Found 3s][Events 0.5s][State 23s][Valid 2s] = 35s/year
Polars Mode: [Init 6s][Found 3s][Events 0.1s][State 23s][Valid 2s] = 35s/year
                                    ‚Üë 0.4s saved (1% improvement)
                                            ‚Üë 23s bottleneck (65%)
```

**The opportunity**: State accumulation is **57√ó slower** than event generation.

### Root Cause

State accumulation uses **dbt incremental models** which:
1. Execute **separate DuckDB queries** for each intermediate model
2. Perform **redundant I/O** reading/writing to disk between models
3. Cannot **leverage Polars' in-memory columnar operations**
4. Miss opportunities for **vectorized transformations**

Example current flow:
```
int_enrollment_state_accumulator.sql (DuckDB)
  ‚Üí Write to disk
  ‚Üí Read from disk
int_deferral_rate_state_accumulator.sql (DuckDB)
  ‚Üí Write to disk
  ‚Üí Read from disk
int_employee_contributions_by_year.sql (DuckDB)
  ‚Üí Write to disk
  ‚Üí Read from disk
fct_workforce_snapshot.sql (DuckDB)
```

---

## Technical Approach

### Architecture

```
planalign_orchestrator/
‚îú‚îÄ‚îÄ polars_event_factory.py        # Event generation (EXISTING - E068G)
‚îú‚îÄ‚îÄ polars_state_pipeline.py       # State accumulation (NEW)
‚îÇ   ‚îú‚îÄ‚îÄ StateAccumulatorEngine
‚îÇ   ‚îú‚îÄ‚îÄ EnrollmentStateBuilder
‚îÇ   ‚îú‚îÄ‚îÄ DeferralRateBuilder
‚îÇ   ‚îú‚îÄ‚îÄ ContributionsCalculator
‚îÇ   ‚îî‚îÄ‚îÄ SnapshotBuilder
‚îî‚îÄ‚îÄ pipeline.py                     # Orchestrator integration (ENHANCED)

dbt/models/
‚îî‚îÄ‚îÄ marts/
    ‚îî‚îÄ‚îÄ fct_workforce_snapshot.sql  # Simplified to read from Polars output
```

### Polars-Native State Pipeline

**Phase 1: Event-to-State Transformation**
```python
# Load events from Parquet (already generated by Polars)
events_df = pl.scan_parquet("data/parquet/events/simulation_year=2025/*.parquet")

# Build enrollment state (parallel)
enrollment_state = events_df.filter(pl.col("event_type") == "benefit_enrollment")
    .group_by(["employee_id", "plan_design_id"])
    .agg([
        pl.col("event_date").max().alias("enrollment_date"),
        pl.col("event_payload").last().alias("latest_payload")
    ])

# Build deferral rate state (parallel)
deferral_state = events_df.filter(pl.col("event_type").is_in(["benefit_enrollment", "deferral_escalation"]))
    .with_columns([
        pl.col("event_payload").str.json_extract("$.initial_deferral_rate").alias("deferral_rate")
    ])
    .group_by(["employee_id"])
    .agg([pl.col("deferral_rate").last()])
```

**Phase 2: Contributions Calculation**
```python
# Vectorized contributions (no SQL, pure Polars)
contributions = baseline_workforce.join(enrollment_state, on="employee_id")
    .join(deferral_state, on="employee_id")
    .with_columns([
        (pl.col("salary") * pl.col("deferral_rate")).alias("employee_contribution"),
        (pl.col("employee_contribution") * match_rate).clip(0, max_match).alias("employer_match")
    ])
```

**Phase 3: Snapshot Generation**
```python
# Final snapshot (single Polars operation)
snapshot = baseline_workforce
    .join(events_df.filter(pl.col("event_type") == "hire"), on="employee_id", how="left")
    .join(enrollment_state, on="employee_id", how="left")
    .join(deferral_state, on="employee_id", how="left")
    .join(contributions, on="employee_id", how="left")
    .with_columns([
        pl.when(pl.col("event_type") == "hire")
          .then(pl.lit("new_hire_active"))
          .when(pl.col("event_type") == "termination")
          .then(pl.lit("terminated"))
          .otherwise(pl.lit("continuous_active"))
          .alias("employee_status")
    ])
```

### Key Performance Optimizations

1. **Zero Disk I/O Between Steps**: All transformations in-memory
2. **Lazy Evaluation**: Query optimization across entire pipeline
3. **Parallel Execution**: Multi-threaded Polars operations
4. **Columnar Processing**: SIMD vectorization for calculations
5. **Streaming Output**: Direct to Parquet without intermediate tables

---

## Stories

### Story S076-01: Polars State Accumulator Foundation (5 points)

**Goal**: Build core Polars state accumulation engine.

**Acceptance Criteria**:
- `StateAccumulatorEngine` class processes events ‚Üí state transformations
- Support enrollment, deferral rate, and contribution state
- Match current dbt output schema exactly
- Performance: <2s for 10k employees

**Deliverables**:
- `planalign_orchestrator/polars_state_pipeline.py`
- Unit tests with 95%+ coverage
- Schema validation against dbt baseline

**Testing**:
```python
# Compare Polars vs dbt output
dbt_snapshot = duckdb.sql("SELECT * FROM fct_workforce_snapshot WHERE simulation_year = 2025")
polars_snapshot = StateAccumulatorEngine(year=2025).build_snapshot()

assert polars_snapshot.equals(dbt_snapshot)  # Exact match required
```

---

### Story S076-02: Enrollment & Deferral State Builders (5 points)

**Goal**: Replace `int_enrollment_state_accumulator.sql` and `int_deferral_rate_state_accumulator.sql` with Polars.

**Acceptance Criteria**:
- Handle temporal state across multiple years
- Support enrollment events, auto-enrollment, opt-outs
- Track deferral escalation over time
- Performance: <500ms per builder

**Deliverables**:
- `EnrollmentStateBuilder` class
- `DeferralRateBuilder` class
- Temporal accumulation logic (Year N uses Year N-1 state)
- Conservation tests (no state loss across years)

---

### Story S076-03: Contributions Calculator (3 points)

**Goal**: Replace `int_employee_contributions_by_year.sql` with vectorized Polars calculations.

**Acceptance Criteria**:
- Support tiered match formulas (simple, tiered, stretch)
- Calculate employee contributions, employer match, core contributions
- Apply eligibility rules
- Performance: <500ms for 10k employees

**Deliverables**:
- `ContributionsCalculator` class
- Match formula implementations
- Eligibility filter logic

---

### Story S076-04: Snapshot Builder (4 points)

**Goal**: Replace `fct_workforce_snapshot.sql` with Polars-based snapshot generation.

**Acceptance Criteria**:
- Combine baseline + events + state ‚Üí final snapshot
- Calculate employee status (continuous_active, new_hire_active, terminated, etc.)
- Include all required columns for reporting
- Performance: <1s for 10k employees

**Deliverables**:
- `SnapshotBuilder` class
- Status classification logic
- Output Parquet files for dbt consumption

---

### Story S076-05: Pipeline Integration & Fallback (5 points)

**Goal**: Integrate Polars state pipeline into PlanAlign Orchestrator with SQL fallback.

**Acceptance Criteria**:
- Configuration flag: `event_generation.polars.state_accumulation_enabled`
- Automatic fallback to dbt on Polars errors
- Performance monitoring and comparison
- Validation against dbt baseline

**Deliverables**:
- `pipeline.py` integration
- Error handling and fallback logic
- Performance benchmarking
- Documentation

**Testing**:
```yaml
# config/simulation_config.yaml
event_generation:
  mode: "polars"
  polars:
    enabled: true
    state_accumulation_enabled: true  # NEW
    fallback_on_error: true
    validate_results: true
```

---

### Story S076-06: Performance Benchmarking & Optimization (3 points)

**Goal**: Benchmark and optimize Polars state pipeline to meet performance targets.

**Acceptance Criteria**:
- State accumulation: <5s per year (vs current 20-25s)
- Total pipeline: <90s for 2-year simulation (vs current 236s)
- Memory usage: <1GB peak
- Generate performance report

**Deliverables**:
- Benchmarking script
- Performance optimization tuning
- Comparison report (Polars vs dbt)
- Recommendations for further optimization

---

## Success Metrics

### Performance Targets

| Metric | Current (dbt) | Target (Polars) | Improvement |
|--------|--------------|-----------------|-------------|
| State Accumulation (per year) | 20-25s | 2-5s | **80-90%** |
| Total Runtime (2-year sim) | 236s | 60-90s | **60-75%** |
| Event Processing Rate | 28k/s | 50-100k/s | **100-250%** |
| Memory Usage | 448MB | <1GB | Acceptable |

### Quality Gates

- ‚úÖ 100% schema compatibility with dbt output
- ‚úÖ Zero data discrepancies in validation tests
- ‚úÖ Graceful fallback to SQL on errors
- ‚úÖ <5% memory overhead vs current baseline

---

## Dependencies

**Prerequisites**:
- ‚úÖ E068G: Polars Bulk Event Factory (COMPLETE)
- ‚úÖ E068B: Incremental State Accumulation (COMPLETE)
- ‚úÖ Polars 1.31.0+ installed

**Blockers**:
- None identified

**Future Enhancements**:
- E077: Polars Validation Pipeline (replace dbt validation)
- E078: End-to-End Polars Simulation (eliminate dbt entirely)

---

## Risks & Mitigations

| Risk | Impact | Probability | Mitigation |
|------|--------|-------------|------------|
| Schema mismatch with dbt | High | Medium | Comprehensive validation tests, exact schema matching |
| Memory overflow with large datasets | High | Low | Streaming operations, lazy evaluation, batch processing |
| Temporal state bugs | Medium | Medium | Conservation tests, year-over-year validation |
| Performance regression | Medium | Low | Benchmarking, fallback to dbt if slower |

---

## Open Questions

1. **dbt Migration Path**: Keep dbt as fallback indefinitely or deprecate?
2. **Parquet Schema Evolution**: How to handle schema changes across versions?
3. **Multi-Scenario Support**: Can Polars handle batch scenario processing efficiently?
4. **Testing Strategy**: Unit tests vs integration tests vs property-based tests?

---

## References

- [E068G: Polars Bulk Event Factory](E068G_polars_bulk_event_factory.md)
- [E068B: Incremental State Accumulation](E068B_incremental_state_accumulation.md)
- [Polars Documentation](https://pola-rs.github.io/polars/py-polars/html/reference/)
- Performance Analysis: 2025-10-07 simulation benchmarks

---

## Notes

**Performance Finding (2025-10-07)**:
```
Polars Mode (E068G only):
  Total: 236s
  Event Generation: 0.1s (28k events/s)
  State Accumulation: 22s ‚Üê BOTTLENECK (70% of runtime)

SQL Mode (baseline):
  Total: 248s
  Event Generation: 0.5s
  State Accumulation: 23s ‚Üê BOTTLENECK (70% of runtime)

Conclusion: Polars event generation is 5√ó faster but only provides 4.6% overall
improvement because state accumulation dominates runtime. E076 targets this
bottleneck for 60-75% total improvement.
```

**Key Insight**: The path to 10-100√ó performance gains requires replacing **the entire state accumulation pipeline** with Polars, not just event generation. This epic makes that leap.
