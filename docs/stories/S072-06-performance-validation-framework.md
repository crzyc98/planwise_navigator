# Story S072-06: Performance & Validation Framework

**Epic**: E021-A - DC Plan Event Schema Foundation
**Story Points**: 8
**Priority**: High
**Sprint**: 3
**Owner**: Platform Team

## Story

**As a** platform engineer
**I want** a comprehensive performance and validation framework for the event schema
**So that** we meet enterprise performance targets and ensure data quality with automated testing

## Business Context\n\nThis story creates the performance and validation infrastructure that ensures the event schema can handle enterprise-scale workloads while maintaining data quality. It establishes performance benchmarks, automated validation, golden dataset testing, and the snapshot strategy needed for production deployment.\n\n## Acceptance Criteria\n\n### Performance Requirements\n- [ ] **Event ingest performance**: ≥100K events/sec using DuckDB vectorized inserts (16-core M2, 32GB)\n- [ ] **History reconstruction**: ≤5s for 5-year participant history (MacBook Pro M3, 16GB)\n- [ ] **Schema validation**: <10ms per event validation with Pydantic v2\n- [ ] **Memory efficiency**: <8GB for 100K employee simulation\n\n### Validation Framework\n- [ ] **JSON schema validation**: ≥99% success rate in CI (build fails if any events are invalid)\n- [ ] **Golden dataset validation**: 100% match with benchmark calculations (zero variance)\n- [ ] **Unit test coverage**: >95% for all 11 payload types with edge case coverage\n- [ ] **Integration test suite**: Complete workflow testing for all event combinations\n\n### Snapshot Strategy\n- [ ] **Weekly balance snapshots** stored in `fct_participant_balance_snapshots`\n- [ ] **Optimized query performance** using pre-computed snapshots\n- [ ] **Event reconstruction fallback** for detailed audit trails\n- [ ] **Snapshot validation** ensuring consistency with event history\n\n### Enterprise Validation\n- [ ] **CI/CD integration** with automated schema validation\n- [ ] **Performance regression testing** with benchmark comparisons\n- [ ] **Data quality monitoring** with automated alerts\n- [ ] **Golden dataset drift detection** with variance analysis\n\n## Technical Specifications\n\n### Performance Testing Framework\n\n```python\nimport time\nimport psutil\nfrom typing import List, Dict, Any\nfrom decimal import Decimal\nfrom datetime import date, datetime\nimport statistics\n\nclass EventPerformanceTester:\n    \"\"\"Performance testing framework for event schema\"\"\"\n    \n    def __init__(self, db_connection):\n        self.db = db_connection\n        self.performance_metrics = {}\n    \n    def test_bulk_event_ingest(self, event_count: int = 100000) -> Dict[str, Any]:\n        \"\"\"Test bulk event ingestion performance\"\"\"\n        \n        # Generate test events\n        test_events = self._generate_test_events(event_count)\n        \n        # Measure memory before\n        process = psutil.Process()\n        memory_before = process.memory_info().rss / 1024 / 1024  # MB\n        \n        # Time the bulk insert\n        start_time = time.perf_counter()\n        \n        # Use DuckDB vectorized insert\n        self._bulk_insert_events(test_events)\n        \n        end_time = time.perf_counter()\n        \n        # Measure memory after\n        memory_after = process.memory_info().rss / 1024 / 1024  # MB\n        \n        duration = end_time - start_time\n        events_per_second = event_count / duration\n        memory_used = memory_after - memory_before\n        \n        metrics = {\n            'event_count': event_count,\n            'duration_seconds': duration,\n            'events_per_second': events_per_second,\n            'memory_used_mb': memory_used,\n            'target_events_per_second': 100000,\n            'target_memory_mb': 8000,\n            'performance_passed': events_per_second >= 100000,\n            'memory_passed': memory_used <= 8000\n        }\n        \n        self.performance_metrics['bulk_ingest'] = metrics\n        return metrics\n    \n    def test_history_reconstruction(self, employee_id: str, years: int = 5) -> Dict[str, Any]:\n        \"\"\"Test participant history reconstruction performance\"\"\"\n        \n        start_date = date.today().replace(year=date.today().year - years)\n        end_date = date.today()\n        \n        start_time = time.perf_counter()\n        \n        # Reconstruct participant history\n        history = self._reconstruct_participant_history(employee_id, start_date, end_date)\n        \n        end_time = time.perf_counter()\n        \n        duration = end_time - start_time\n        event_count = len(history)\n        \n        metrics = {\n            'employee_id': employee_id,\n            'reconstruction_time_seconds': duration,\n            'event_count': event_count,\n            'target_time_seconds': 5.0,\n            'reconstruction_passed': duration <= 5.0\n        }\n        \n        self.performance_metrics['history_reconstruction'] = metrics\n        return metrics\n    \n    def test_schema_validation_performance(self, event_count: int = 10000) -> Dict[str, Any]:\n        \"\"\"Test individual event validation performance\"\"\"\n        \n        test_events = self._generate_test_events(event_count)\n        validation_times = []\n        \n        for event_data in test_events:\n            start_time = time.perf_counter()\n            \n            # Validate single event\n            event = SimulationEvent.model_validate(event_data)\n            \n            end_time = time.perf_counter()\n            validation_times.append((end_time - start_time) * 1000)  # Convert to ms\n        \n        avg_validation_time = statistics.mean(validation_times)\n        max_validation_time = max(validation_times)\n        \n        metrics = {\n            'event_count': event_count,\n            'avg_validation_time_ms': avg_validation_time,\n            'max_validation_time_ms': max_validation_time,\n            'target_validation_time_ms': 10.0,\n            'validation_passed': avg_validation_time <= 10.0\n        }\n        \n        self.performance_metrics['schema_validation'] = metrics\n        return metrics\n    \n    def _generate_test_events(self, count: int) -> List[Dict[str, Any]]:\n        \"\"\"Generate test events for performance testing\"\"\"\n        events = []\n        \n        for i in range(count):\n            # Generate different event types for realistic testing\n            event_type = self._get_event_type_for_index(i)\n            event_data = self._generate_event_data(event_type, i)\n            events.append(event_data)\n        \n        return events\n    \n    def _bulk_insert_events(self, events: List[Dict[str, Any]]) -> None:\n        \"\"\"Bulk insert events using DuckDB vectorized operations\"\"\"\n        # Convert events to DataFrame for vectorized insert\n        import pandas as pd\n        \n        df = pd.DataFrame(events)\n        \n        # Use DuckDB vectorized insert\n        self.db.execute(\n            \"INSERT INTO fct_yearly_events SELECT * FROM df\"\n        )\n    \n    def _reconstruct_participant_history(\n        self, \n        employee_id: str, \n        start_date: date, \n        end_date: date\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Reconstruct complete participant history from events\"\"\"\n        \n        query = \"\"\"\n            SELECT *\n            FROM fct_yearly_events\n            WHERE employee_id = ?\n                AND effective_date BETWEEN ? AND ?\n            ORDER BY effective_date, created_at\n        \"\"\"\n        \n        result = self.db.execute(query, [employee_id, start_date, end_date])\n        return [dict(row) for row in result.fetchall()]\n```\n\n### Golden Dataset Validation Framework\n\n```python\nclass GoldenDatasetValidator:\n    \"\"\"Validates event processing against golden benchmark datasets\"\"\"\n    \n    def __init__(self, golden_dataset_path: str):\n        self.golden_dataset_path = golden_dataset_path\n        self.validation_results = {}\n    \n    def validate_against_golden_dataset(self, test_results: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"Validate test results against golden benchmark\"\"\"\n        \n        golden_data = self._load_golden_dataset()\n        \n        validation_results = {\n            'total_records': len(golden_data),\n            'matched_records': 0,\n            'discrepancies': [],\n            'max_variance': Decimal('0'),\n            'variance_threshold': Decimal('0.01'),  # 1 cent tolerance\n            'is_benchmark_match': True\n        }\n        \n        # Create lookup map for test results\n        test_map = {\n            (result['employee_id'], result['effective_date']): result\n            for result in test_results\n        }\n        \n        for golden_record in golden_data:\n            employee_id = golden_record['employee_id']\n            effective_date = golden_record['effective_date']\n            expected_amount = Decimal(str(golden_record['expected_amount']))\n            \n            test_result = test_map.get((employee_id, effective_date))\n            \n            if not test_result:\n                validation_results['discrepancies'].append({\n                    'employee_id': employee_id,\n                    'effective_date': str(effective_date),\n                    'error': 'Missing test result'\n                })\n                validation_results['is_benchmark_match'] = False\n                continue\n            \n            actual_amount = Decimal(str(test_result['actual_amount']))\n            variance = abs(actual_amount - expected_amount)\n            validation_results['max_variance'] = max(validation_results['max_variance'], variance)\n            \n            if variance > validation_results['variance_threshold']:\n                validation_results['discrepancies'].append({\n                    'employee_id': employee_id,\n                    'effective_date': str(effective_date),\n                    'expected': float(expected_amount),\n                    'actual': float(actual_amount),\n                    'variance': float(variance)\n                })\n                validation_results['is_benchmark_match'] = False\n            else:\n                validation_results['matched_records'] += 1\n        \n        self.validation_results = validation_results\n        return validation_results\n    \n    def _load_golden_dataset(self) -> List[Dict[str, Any]]:\n        \"\"\"Load golden dataset from file\"\"\"\n        import json\n        \n        with open(self.golden_dataset_path, 'r') as f:\n            return json.load(f)\n```\n\n### Snapshot Strategy Implementation\n\n```python\nclass SnapshotManager:\n    \"\"\"Manages participant balance snapshots for performance optimization\"\"\"\n    \n    def __init__(self, db_connection):\n        self.db = db_connection\n        self._initialize_snapshot_tables()\n    \n    def _initialize_snapshot_tables(self):\n        \"\"\"Create snapshot tables if they don't exist\"\"\"\n        \n        self.db.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS fct_participant_balance_snapshots (\n                snapshot_id UUID PRIMARY KEY,\n                employee_id VARCHAR NOT NULL,\n                plan_id VARCHAR NOT NULL,\n                snapshot_date DATE NOT NULL,\n                \n                -- Balance by source\n                employee_pre_tax_balance DECIMAL(18,6) DEFAULT 0,\n                employee_roth_balance DECIMAL(18,6) DEFAULT 0,\n                employee_after_tax_balance DECIMAL(18,6) DEFAULT 0,\n                employer_match_balance DECIMAL(18,6) DEFAULT 0,\n                employer_nonelective_balance DECIMAL(18,6) DEFAULT 0,\n                employer_profit_sharing_balance DECIMAL(18,6) DEFAULT 0,\n                \n                -- Vesting information\n                vested_percentage DECIMAL(5,4) DEFAULT 1.0,\n                vested_balance DECIMAL(18,6) DEFAULT 0,\n                \n                -- Loan information\n                outstanding_loan_balance DECIMAL(18,6) DEFAULT 0,\n                \n                -- Metadata\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                event_count_since_last_snapshot INTEGER DEFAULT 0,\n                \n                INDEX idx_employee_date (employee_id, snapshot_date),\n                INDEX idx_plan_date (plan_id, snapshot_date)\n            )\n        \"\"\")\n    \n    def create_weekly_snapshots(self, as_of_date: date) -> Dict[str, Any]:\n        \"\"\"Create weekly balance snapshots for all participants\"\"\"\n        \n        start_time = time.perf_counter()\n        \n        # Get all active participants\n        participants = self._get_active_participants(as_of_date)\n        \n        snapshots_created = 0\n        \n        for participant in participants:\n            employee_id = participant['employee_id']\n            plan_id = participant['plan_id']\n            \n            # Calculate balance from events\n            balance_data = self._calculate_balance_from_events(\n                employee_id, plan_id, as_of_date\n            )\n            \n            # Create snapshot record\n            self._insert_snapshot(employee_id, plan_id, as_of_date, balance_data)\n            snapshots_created += 1\n        \n        end_time = time.perf_counter()\n        \n        return {\n            'snapshots_created': snapshots_created,\n            'processing_time_seconds': end_time - start_time,\n            'snapshot_date': str(as_of_date)\n        }\n    \n    def get_participant_balance(\n        self, \n        employee_id: str, \n        plan_id: str, \n        as_of_date: date\n    ) -> Dict[str, Any]:\n        \"\"\"Get participant balance using snapshot strategy\"\"\"\n        \n        # Find most recent snapshot before or on as_of_date\n        snapshot = self._get_latest_snapshot(employee_id, plan_id, as_of_date)\n        \n        if not snapshot:\n            # No snapshot available, reconstruct from events\n            return self._calculate_balance_from_events(employee_id, plan_id, as_of_date)\n        \n        snapshot_date = snapshot['snapshot_date']\n        \n        if snapshot_date == as_of_date:\n            # Snapshot is for exact date requested\n            return snapshot\n        \n        # Apply events since snapshot\n        events_since_snapshot = self._get_events_since_snapshot(\n            employee_id, plan_id, snapshot_date, as_of_date\n        )\n        \n        # Update snapshot with recent events\n        updated_balance = self._apply_events_to_snapshot(snapshot, events_since_snapshot)\n        \n        return updated_balance\n```\n\n### CI/CD Integration\n\n```yaml\n# .github/workflows/event-schema-validation.yml\nname: Event Schema Validation\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  schema-validation:\n    runs-on: ubuntu-latest\n    \n    steps:\n    - uses: actions/checkout@v3\n    \n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.11'\n    \n    - name: Install dependencies\n      run: |\n        pip install -r requirements.txt\n        pip install pytest pytest-cov\n    \n    - name: Run Schema Validation Tests\n      run: |\n        pytest tests/schema/ -v --cov=orchestrator/events --cov-report=xml\n    \n    - name: Run Performance Tests\n      run: |\n        pytest tests/performance/ -v --benchmark-only\n    \n    - name: Validate Against Golden Dataset\n      run: |\n        python scripts/validate_golden_dataset.py\n    \n    - name: Check Coverage\n      run: |\n        coverage report --fail-under=95\n```\n\n## Implementation Tasks\n\n### Phase 1: Performance Testing Framework\n- [ ] **Create EventPerformanceTester** with bulk ingest testing\n- [ ] **Implement history reconstruction** performance testing\n- [ ] **Add schema validation** performance measurement\n- [ ] **Create memory efficiency** monitoring\n\n### Phase 2: Validation Framework\n- [ ] **Create GoldenDatasetValidator** with variance checking\n- [ ] **Implement CI/CD integration** with automated validation\n- [ ] **Add JSON schema validation** with 99% success requirement\n- [ ] **Create data quality monitoring** with automated alerts\n\n### Phase 3: Snapshot Strategy\n- [ ] **Create SnapshotManager** with weekly snapshot creation\n- [ ] **Implement balance calculation** from event history\n- [ ] **Add snapshot-based queries** for performance optimization\n- [ ] **Create snapshot validation** ensuring consistency\n\n### Phase 4: Enterprise Integration\n- [ ] **Add performance regression testing** with benchmark comparisons\n- [ ] **Create comprehensive test suite** for all 11 event types\n- [ ] **Implement monitoring dashboards** for production metrics\n- [ ] **Add automated alerting** for performance degradation\n\n## Dependencies\n\n### Story Dependencies\n- **S072-01**: Core Event Model (blocking)\n- **S072-02**: Workforce Events (blocking)\n- **S072-03**: Core DC Plan Events (blocking)\n- **S072-04**: Plan Administration Events (blocking)\n\n\n### Infrastructure Dependencies\n- **DuckDB performance features**: Vectorized operations\n- **CI/CD pipeline**: Automated testing integration\n- **Monitoring infrastructure**: Performance metrics collection\n- **Golden dataset**: Benchmark validation data\n\n## Success Metrics\n\n### Performance Targets\n- [ ] **≥100K events/sec ingest** on specified hardware\n- [ ] **≤5s history reconstruction** for 5-year participant history\n- [ ] **<10ms schema validation** per event\n- [ ] **<8GB memory usage** for 100K employee simulation\n\n### Quality Targets\n- [ ] **≥99% CI validation success** (build fails below threshold)\n- [ ] **100% golden dataset match** (zero variance tolerance)\n- [ ] **>95% unit test coverage** for all payload types\n- [ ] **Zero performance regression** in production deployment\n\n## Testing Strategy\n\n### Performance Tests\n- [ ] **Bulk ingest testing** with 100K+ events\n- [ ] **History reconstruction** with realistic participant data\n- [ ] **Memory profiling** under high load\n- [ ] **Concurrent access** testing with multiple scenarios\n\n### Validation Tests\n- [ ] **Golden dataset validation** with benchmark scenarios\n- [ ] **Schema compliance** testing for all 11 event types\n- [ ] **Edge case validation** with boundary conditions\n- [ ] **Regression testing** against previous versions\n\n### Integration Tests\n- [ ] **End-to-end workflow** testing with complete participant lifecycle\n- [ ] **Snapshot consistency** validation with event reconstruction\n- [ ] **CI/CD pipeline** testing with automated validation\n- [ ] **Production deployment** validation with performance monitoring\n\n## Definition of Done\n\n- [ ] **Performance framework implemented** with automated testing\n- [ ] **All performance targets met** on specified hardware configurations\n- [ ] **Golden dataset validation** achieving 100% benchmark match\n- [ ] **CI/CD integration complete** with ≥99% validation success\n- [ ] **Snapshot strategy implemented** with weekly balance snapshots\n- [ ] **Comprehensive test coverage** >95% for all event types\n- [ ] **Documentation complete** with performance benchmarks and validation procedures\n- [ ] **Production monitoring** ready with automated alerting\n\n## Notes\n\nThis story creates the foundation for enterprise-scale deployment by ensuring the event schema can handle production workloads while maintaining data quality. The performance and validation framework will be critical for maintaining system reliability as the DC plan functionality expands.
