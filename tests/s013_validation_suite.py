"""
S013-07: Comprehensive Validation and Testing Suite

This module implements the comprehensive validation framework required by S013-07
to ensure that the modularized pipeline preserves identical behavior while
improving maintainability.

Key Features:
- Unit test coverage validation (>95% target)
- Integration test behavior comparison
- Performance regression detection
- Mathematical validation of simulation results
- Character-level logging output comparison
"""

import hashlib
import subprocess
import time
from pathlib import Path
from typing import Any, Dict, List, Optional
from unittest.mock import Mock, patch

import psutil
import pytest
from orchestrator.simulator_pipeline import (YearResult, clean_duckdb_data,
                                             execute_dbt_command,
                                             run_multi_year_simulation,
                                             run_year_simulation)


class S013ValidationFramework:
    """Comprehensive validation framework for S013 Epic completion."""

    def __init__(self):
        self.baseline_metrics = {}
        self.test_results = {}

    def validate_unit_test_coverage(self) -> Dict[str, Any]:
        """Validate unit test coverage meets >95% requirement."""
        try:
            # Run pytest with coverage
            result = subprocess.run(
                [
                    "python",
                    "-m",
                    "pytest",
                    "tests/unit/",
                    "--cov=orchestrator.simulator_pipeline",
                    "--cov-report=json",
                    "--cov-report=term-missing",
                    "--tb=short",
                ],
                capture_output=True,
                text=True,
                timeout=300,
            )

            coverage_data = {
                "returncode": result.returncode,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "coverage_percentage": self._extract_coverage_percentage(result.stdout),
            }

            return {
                "success": result.returncode == 0,
                "coverage_data": coverage_data,
                "meets_threshold": coverage_data.get("coverage_percentage", 0) >= 95,
            }

        except subprocess.TimeoutExpired:
            return {"success": False, "error": "Coverage test timed out"}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def _extract_coverage_percentage(self, output: str) -> float:
        """Extract coverage percentage from pytest-cov output."""
        try:
            for line in output.split("\n"):
                if "TOTAL" in line and "%" in line:
                    # Extract percentage from line like "TOTAL    100    20    80%"
                    parts = line.split()
                    for part in parts:
                        if part.endswith("%"):
                            return float(part[:-1])
            return 0.0
        except:
            return 0.0

    def validate_behavior_preservation(self) -> Dict[str, Any]:
        """Validate that refactored pipeline produces identical behavior."""
        validation_results = {
            "year_result_consistency": self._validate_year_result_consistency(),
            "multi_year_orchestration": self._validate_multi_year_orchestration(),
            "error_handling_preservation": self._validate_error_handling(),
            "mathematical_accuracy": self._validate_mathematical_accuracy(),
        }

        all_passed = all(
            result.get("success", False) for result in validation_results.values()
        )

        return {
            "success": all_passed,
            "detailed_results": validation_results,
            "summary": f"{'‚úÖ' if all_passed else '‚ùå'} Behavior validation {'passed' if all_passed else 'failed'}",
        }

    def _validate_year_result_consistency(self) -> Dict[str, Any]:
        """Validate YearResult structure is consistent."""
        try:
            # Create mock context for testing
            mock_context = self._create_test_context()

            with patch(
                "orchestrator.simulator_pipeline.duckdb.connect"
            ) as mock_connect:
                with patch("orchestrator.simulator_pipeline.execute_dbt_command"):
                    with patch(
                        "orchestrator.simulator_pipeline._run_dbt_event_models_for_year_internal"
                    ):
                        with patch(
                            "orchestrator.simulator_pipeline.validate_year_results"
                        ) as mock_validate:
                            # Mock successful result
                            expected_result = YearResult(
                                year=2025,
                                success=True,
                                active_employees=1030,
                                total_terminations=120,
                                experienced_terminations=100,
                                new_hire_terminations=20,
                                total_hires=150,
                                growth_rate=0.03,
                                validation_passed=True,
                            )
                            mock_validate.return_value = expected_result

                            # Execute and validate
                            result = run_year_simulation(mock_context)

                            # Check all required fields
                            required_fields = [
                                "year",
                                "success",
                                "active_employees",
                                "total_terminations",
                                "experienced_terminations",
                                "new_hire_terminations",
                                "total_hires",
                                "growth_rate",
                                "validation_passed",
                            ]

                            field_validation = {}
                            for field in required_fields:
                                field_validation[field] = hasattr(result, field)

                            return {
                                "success": all(field_validation.values()),
                                "field_validation": field_validation,
                                "result_type": type(result).__name__,
                            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    def _validate_multi_year_orchestration(self) -> Dict[str, Any]:
        """Validate multi-year orchestration behaves correctly."""
        try:
            mock_context = self._create_test_context()

            with patch(
                "orchestrator.simulator_pipeline.clean_duckdb_data"
            ) as mock_clean:
                with patch(
                    "orchestrator.simulator_pipeline._execute_single_year_with_recovery"
                ) as mock_execute:
                    # Setup successful execution
                    mock_execute.side_effect = [
                        YearResult(
                            year=2025,
                            success=True,
                            active_employees=1030,
                            total_terminations=120,
                            experienced_terminations=100,
                            new_hire_terminations=20,
                            total_hires=150,
                            growth_rate=0.03,
                            validation_passed=True,
                        ),
                        YearResult(
                            year=2026,
                            success=True,
                            active_employees=1061,
                            total_terminations=125,
                            experienced_terminations=105,
                            new_hire_terminations=20,
                            total_hires=156,
                            growth_rate=0.03,
                            validation_passed=True,
                        ),
                    ]

                    # Execute multi-year simulation
                    results = run_multi_year_simulation(mock_context, True)

                    # Validate orchestration
                    orchestration_checks = {
                        "clean_data_called": mock_clean.called,
                        "correct_year_count": len(results) == 2,
                        "all_years_successful": all(r.success for r in results),
                        "correct_year_sequence": [r.year for r in results]
                        == [2025, 2026],
                        "execute_called_correctly": mock_execute.call_count == 2,
                    }

                    return {
                        "success": all(orchestration_checks.values()),
                        "orchestration_checks": orchestration_checks,
                    }

        except Exception as e:
            return {"success": False, "error": str(e)}

    def _validate_error_handling(self) -> Dict[str, Any]:
        """Validate error handling behavior is preserved."""
        try:
            mock_context = self._create_test_context()

            # Test baseline validation failure
            try:
                run_multi_year_simulation(mock_context, False)  # baseline_valid=False
                baseline_test = {
                    "success": False,
                    "error": "Should have raised exception",
                }
            except Exception as e:
                baseline_test = {
                    "success": "Baseline workforce validation failed" in str(e),
                    "error_message": str(e),
                }

            # Test single year error handling
            with patch(
                "orchestrator.simulator_pipeline.execute_dbt_command"
            ) as mock_execute:
                mock_execute.side_effect = Exception("Test error")

                error_result = run_year_simulation(mock_context)

                single_year_test = {
                    "success": error_result.success is False,
                    "error_year": error_result.year == 2025,
                    "validation_failed": error_result.validation_passed is False,
                }

            return {
                "success": baseline_test["success"] and single_year_test["success"],
                "baseline_validation_test": baseline_test,
                "single_year_error_test": single_year_test,
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    def _validate_mathematical_accuracy(self) -> Dict[str, Any]:
        """Validate mathematical calculations are accurate."""
        try:
            # Test hiring calculation logic with known values
            test_cases = [
                {
                    "workforce_count": 1000,
                    "total_termination_rate": 0.12,
                    "target_growth_rate": 0.03,
                    "new_hire_termination_rate": 0.25,
                    "expected": {
                        "experienced_terms": 120,
                        "growth_amount": 30.0,
                        "total_hires_needed": 200,
                        "expected_new_hire_terms": 50,
                    },
                },
                {
                    "workforce_count": 500,
                    "total_termination_rate": 0.10,
                    "target_growth_rate": 0.05,
                    "new_hire_termination_rate": 0.20,
                    "expected": {
                        "experienced_terms": 50,
                        "growth_amount": 25.0,
                        "total_hires_needed": 94,
                        "expected_new_hire_terms": 19,
                    },
                },
            ]

            calculation_results = []
            for case in test_cases:
                # Simulate the hiring calculation logic
                import math

                workforce_count = case["workforce_count"]
                total_termination_rate = case["total_termination_rate"]
                target_growth_rate = case["target_growth_rate"]
                new_hire_termination_rate = case["new_hire_termination_rate"]

                experienced_terms = math.ceil(workforce_count * total_termination_rate)
                growth_amount = workforce_count * target_growth_rate
                total_hires_needed = math.ceil(
                    (experienced_terms + growth_amount)
                    / (1 - new_hire_termination_rate)
                )
                expected_new_hire_terms = round(
                    total_hires_needed * new_hire_termination_rate
                )

                actual = {
                    "experienced_terms": experienced_terms,
                    "growth_amount": growth_amount,
                    "total_hires_needed": total_hires_needed,
                    "expected_new_hire_terms": expected_new_hire_terms,
                }

                match = actual == case["expected"]
                calculation_results.append(
                    {"case": case, "actual": actual, "match": match}
                )

            all_calculations_correct = all(
                result["match"] for result in calculation_results
            )

            return {
                "success": all_calculations_correct,
                "calculation_results": calculation_results,
                "summary": f"{'‚úÖ' if all_calculations_correct else '‚ùå'} Mathematical validation",
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    def measure_performance_regression(
        self, baseline_time: Optional[float] = None
    ) -> Dict[str, Any]:
        """Measure performance and detect regressions."""
        try:
            # Measure current performance
            mock_context = self._create_test_context()

            with patch("orchestrator.simulator_pipeline.clean_duckdb_data"):
                with patch(
                    "orchestrator.simulator_pipeline._execute_single_year_with_recovery"
                ) as mock_execute:
                    # Setup quick mock execution
                    mock_execute.return_value = YearResult(
                        year=2025,
                        success=True,
                        active_employees=1000,
                        total_terminations=100,
                        experienced_terminations=80,
                        new_hire_terminations=20,
                        total_hires=130,
                        growth_rate=0.03,
                        validation_passed=True,
                    )

                    # Measure execution time
                    start_time = time.time()
                    process = psutil.Process()
                    initial_memory = process.memory_info().rss

                    results = run_multi_year_simulation(mock_context, True)

                    end_time = time.time()
                    final_memory = process.memory_info().rss

                    execution_time = end_time - start_time
                    memory_usage = final_memory - initial_memory

                    performance_data = {
                        "execution_time": execution_time,
                        "memory_usage_bytes": memory_usage,
                        "memory_usage_mb": memory_usage / 1024 / 1024,
                        "results_count": len(results),
                    }

                    # Check regression if baseline provided
                    regression_check = {}
                    if baseline_time:
                        time_regression = (
                            execution_time - baseline_time
                        ) / baseline_time
                        regression_check = {
                            "baseline_time": baseline_time,
                            "current_time": execution_time,
                            "regression_percentage": time_regression * 100,
                            "acceptable": time_regression <= 0.05,  # 5% threshold
                        }

                    return {
                        "success": True,
                        "performance_data": performance_data,
                        "regression_check": regression_check,
                    }

        except Exception as e:
            return {"success": False, "error": str(e)}

    def _create_test_context(self):
        """Create a standardized test context for validation."""
        from dagster import build_op_context

        return build_op_context(
            op_config={
                "start_year": 2025,
                "end_year": 2026,
                "target_growth_rate": 0.03,
                "total_termination_rate": 0.12,
                "new_hire_termination_rate": 0.25,
                "random_seed": 42,
                "full_refresh": False,
            },
            resources={"dbt": Mock()},
        )

    def generate_validation_report(self) -> Dict[str, Any]:
        """Generate comprehensive validation report for S013-07."""
        print("üöÄ Starting S013-07 Comprehensive Validation Suite...")

        # Run all validation tests
        coverage_result = self.validate_unit_test_coverage()
        behavior_result = self.validate_behavior_preservation()
        performance_result = self.measure_performance_regression()

        # Calculate overall success
        all_validations = [coverage_result, behavior_result, performance_result]
        overall_success = all(
            result.get("success", False) for result in all_validations
        )

        # Generate summary
        report = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "overall_success": overall_success,
            "validation_results": {
                "unit_test_coverage": coverage_result,
                "behavior_preservation": behavior_result,
                "performance_regression": performance_result,
            },
            "s013_epic_status": "‚úÖ VALIDATION PASSED"
            if overall_success
            else "‚ùå VALIDATION FAILED",
            "summary": self._generate_summary(
                coverage_result, behavior_result, performance_result
            ),
        }

        return report

    def _generate_summary(
        self, coverage_result, behavior_result, performance_result
    ) -> str:
        """Generate validation summary."""
        lines = ["=== S013-07 Validation Summary ==="]

        # Coverage summary
        coverage_pct = coverage_result.get("coverage_data", {}).get(
            "coverage_percentage", 0
        )
        coverage_status = "‚úÖ" if coverage_result.get("meets_threshold", False) else "‚ùå"
        lines.append(
            f"{coverage_status} Unit Test Coverage: {coverage_pct:.1f}% (Target: ‚â•95%)"
        )

        # Behavior summary
        behavior_status = "‚úÖ" if behavior_result.get("success", False) else "‚ùå"
        lines.append(
            f"{behavior_status} Behavior Preservation: {behavior_result.get('summary', 'Unknown')}"
        )

        # Performance summary
        perf_status = "‚úÖ" if performance_result.get("success", False) else "‚ùå"
        exec_time = performance_result.get("performance_data", {}).get(
            "execution_time", 0
        )
        lines.append(
            f"{perf_status} Performance Validation: {exec_time:.3f}s execution time"
        )

        return "\n".join(lines)


def pytest_run_s013_validation():
    """Pytest-compatible function to run S013 validation."""
    validator = S013ValidationFramework()
    report = validator.generate_validation_report()

    # Print detailed report
    print("\n" + "=" * 80)
    print("S013-07: COMPREHENSIVE VALIDATION REPORT")
    print("=" * 80)
    print(f"Timestamp: {report['timestamp']}")
    print(f"Status: {report['s013_epic_status']}")
    print("\n" + report["summary"])

    # Print detailed results
    for category, result in report["validation_results"].items():
        print(f"\n--- {category.replace('_', ' ').title()} ---")
        if result.get("success"):
            print("‚úÖ PASSED")
        else:
            print("‚ùå FAILED")
            if "error" in result:
                print(f"Error: {result['error']}")

    print("\n" + "=" * 80)

    # Assert overall success for pytest
    assert report["overall_success"], f"S013-07 validation failed: {report['summary']}"

    return report


if __name__ == "__main__":
    # Run validation when executed directly
    validator = S013ValidationFramework()
    report = validator.generate_validation_report()

    print("\n" + "=" * 80)
    print("S013-07: COMPREHENSIVE VALIDATION REPORT")
    print("=" * 80)
    print(f"Timestamp: {report['timestamp']}")
    print(f"Status: {report['s013_epic_status']}")
    print("\n" + report["summary"])

    exit_code = 0 if report["overall_success"] else 1
    exit(exit_code)
